{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db5d1ebf-6d81-4d1d-a1fb-102595820d06",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (25.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mWriting to /root/.config/pip/pip.conf\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: optimum[neuronx] in /opt/conda/lib/python3.10/site-packages (1.23.3)\n",
      "Collecting optimum[neuronx]\n",
      "  Using cached optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: transformers>=4.29 in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (4.48.3)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (2.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (1.25.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (0.30.1)\n",
      "Requirement already satisfied: optimum-neuron>=0.0.20 in /opt/conda/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.1.0)\n",
      "Collecting transformers>=4.29 (from optimum[neuronx])\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[neuronx]) (4.13.0)\n",
      "INFO: pip is looking at multiple versions of optimum-neuron to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting optimum-neuron>=0.0.20 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached optimum_neuron-0.0.28-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached optimum_neuron-0.0.27-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached optimum_neuron-0.0.26-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached optimum_neuron-0.0.25-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached optimum_neuron-0.0.24-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting transformers>=4.29 (from optimum[neuronx])\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: accelerate==0.29.2 in /opt/conda/lib/python3.10/site-packages (from optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.29.2)\n",
      "Collecting optimum-neuron>=0.0.20 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached optimum_neuron-0.0.23-py3-none-any.whl.metadata (14 kB)\n",
      "  Using cached optimum_neuron-0.0.22-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting transformers>=4.29 (from optimum[neuronx])\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Collecting accelerate==0.23.0 (from optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "INFO: pip is still looking at multiple versions of optimum-neuron to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting optimum-neuron>=0.0.20 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached optimum_neuron-0.0.21-py3-none-any.whl.metadata (14 kB)\n",
      "  Using cached optimum_neuron-0.0.20-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (1.13.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum[neuronx]) (2.19.1)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting optimum[neuronx]\n",
      "  Using cached optimum-1.23.3-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached optimum-1.23.2-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached optimum-1.23.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached optimum-1.23.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Using cached optimum-1.22.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers<4.45.0,>=4.29 (from transformers[sentencepiece]<4.45.0,>=4.29->optimum[neuronx])\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting optimum[neuronx]\n",
      "  Using cached optimum-1.21.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers<4.44.0,>=4.29.0 (from transformers[sentencepiece]<4.44.0,>=4.29.0->optimum[neuronx])\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting optimum[neuronx]\n",
      "  Using cached optimum-1.21.3-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached optimum-1.21.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers<4.43.0,>=4.26.0 (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum[neuronx])\n",
      "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting optimum[neuronx]\n",
      "  Using cached optimum-1.21.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached optimum-1.21.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached optimum-1.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: protobuf<4 in /opt/conda/lib/python3.10/site-packages (from optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.29->optimum[neuronx]) (2024.11.6)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.29->optimum[neuronx])\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.29->optimum[neuronx]) (0.5.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.2->optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (7.0.0)\n",
      "INFO: pip is looking at multiple versions of optimum-neuron[neuronx] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.45.1)\n",
      "Collecting neuronx-cc==2.14.227.0 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/neuronx-cc/neuronx_cc-2.14.227.0%2B2d4f85be-cp310-cp310-linux_x86_64.whl (392.3 MB)\n",
      "Collecting torch-neuronx==2.1.2.2.2.0 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/torch-neuronx/torch_neuronx-2.1.2.2.2.0-py3-none-any.whl (2.5 MB)\n",
      "Collecting transformers-neuronx==0.11.351 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/transformers-neuronx/transformers_neuronx-0.11.351-py3-none-any.whl (240 kB)\n",
      "Collecting torch>=1.11 (from optimum[neuronx])\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision==0.16.* (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting neuronx-distributed==0.8.0 (from optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/neuronx-distributed/neuronx_distributed-0.8.0-py3-none-linux_x86_64.whl (172 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum[neuronx]) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch>=1.11->optimum[neuronx])\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: scipy<=1.11.2 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.11.2)\n",
      "Requirement already satisfied: python-daemon>=2.2.4 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.1.2)\n",
      "Requirement already satisfied: requests-unixsocket>=0.1.5 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.4.1)\n",
      "Requirement already satisfied: islpy~=2023.1 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2023.2.5)\n",
      "Collecting protobuf<4 (from optimum-neuron>=0.0.20->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
      "Requirement already satisfied: pgzip>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.3.5)\n",
      "Requirement already satisfied: ec2-metadata~=2.10 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.14.0)\n",
      "Requirement already satisfied: torch-xla in /opt/conda/lib/python3.10/site-packages (from neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.5.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum[neuronx]) (12.4.127)\n",
      "Collecting torch-xla (from neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx])\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/torch-xla/torch_xla-2.1.3-cp310-cp310-manylinux_2_28_x86_64.whl (81.1 MB)\n",
      "Requirement already satisfied: libneuronxla<3.0,>2.0.965 in /opt/conda/lib/python3.10/site-packages (from torch-neuronx==2.1.2.2.2.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.2.1630.0)\n",
      "Requirement already satisfied: setuptools<=69.5.1 in /opt/conda/lib/python3.10/site-packages (from torch-neuronx==2.1.2.2.2.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (69.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.*->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (11.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.2.2)\n",
      "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.10)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum[neuronx]) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum[neuronx]) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum[neuronx]) (3.11.16)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum[neuronx]) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum[neuronx]) (1.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum[neuronx]) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum[neuronx]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[neuronx]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum[neuronx]) (2025.2)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from libneuronxla<3.0,>2.0.965->torch-neuronx==2.1.2.2.2.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.37.26)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.10/site-packages (from libneuronxla<3.0,>2.0.965->torch-neuronx==2.1.2.2.2.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.37.26)\n",
      "Requirement already satisfied: lockfile>=0.10 in /opt/conda/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.14.227.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[neuronx]) (1.17.0)\n",
      "Requirement already satisfied: google-api-python-client==1.8.0 in /opt/conda/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.8.0)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (4.1.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.22.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (2.38.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.2.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.34.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.0.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->libneuronxla<3.0,>2.0.965->torch-neuronx==2.1.2.2.2.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from boto3->libneuronxla<3.0,>2.0.965->torch-neuronx==2.1.2.2.2.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.11.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (0.4.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (4.7.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (1.63.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (5.5.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.8.0->optimum-neuron[neuronx]>=0.0.20; extra == \"neuronx\"->optimum[neuronx]) (3.2.3)\n",
      "Using cached optimum_neuron-0.0.24-py3-none-any.whl (345 kB)\n",
      "Using cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Using cached optimum-1.20.0-py3-none-any.whl (418 kB)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: triton, protobuf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, neuronx-cc, transformers, torchvision, torch-xla, optimum, torch-neuronx, optimum-neuron, transformers-neuronx, neuronx-distributed\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: neuronx-cc\n",
      "    Found existing installation: neuronx-cc 2.17.194.0+d312836f\n",
      "    Uninstalling neuronx-cc-2.17.194.0+d312836f:\n",
      "      Successfully uninstalled neuronx-cc-2.17.194.0+d312836f\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.3\n",
      "    Uninstalling transformers-4.48.3:\n",
      "      Successfully uninstalled transformers-4.48.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1\n",
      "    Uninstalling torchvision-0.20.1:\n",
      "      Successfully uninstalled torchvision-0.20.1\n",
      "  Attempting uninstall: torch-xla\n",
      "    Found existing installation: torch-xla 2.5.1\n",
      "    Uninstalling torch-xla-2.5.1:\n",
      "      Successfully uninstalled torch-xla-2.5.1\n",
      "  Attempting uninstall: optimum\n",
      "    Found existing installation: optimum 1.23.3\n",
      "    Uninstalling optimum-1.23.3:\n",
      "      Successfully uninstalled optimum-1.23.3\n",
      "  Attempting uninstall: torch-neuronx\n",
      "    Found existing installation: torch-neuronx 2.5.1.2.6.0\n",
      "    Uninstalling torch-neuronx-2.5.1.2.6.0:\n",
      "      Successfully uninstalled torch-neuronx-2.5.1.2.6.0\n",
      "  Attempting uninstall: optimum-neuron\n",
      "    Found existing installation: optimum-neuron 0.1.0\n",
      "    Uninstalling optimum-neuron-0.1.0:\n",
      "      Successfully uninstalled optimum-neuron-0.1.0\n",
      "  Attempting uninstall: transformers-neuronx\n",
      "    Found existing installation: transformers-neuronx 0.13.322\n",
      "    Uninstalling transformers-neuronx-0.13.322:\n",
      "      Successfully uninstalled transformers-neuronx-0.13.322\n",
      "  Attempting uninstall: neuronx-distributed\n",
      "    Found existing installation: neuronx-distributed 0.11.0\n",
      "    Uninstalling neuronx-distributed-0.11.0:\n",
      "      Successfully uninstalled neuronx-distributed-0.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "neuronx-distributed-inference 0.2.0 requires blobfile, which is not installed.\n",
      "neuronx-distributed-inference 0.2.0 requires transformers==4.48.*, but you have transformers 4.41.1 which is incompatible.\n",
      "vllm 0.1.dev4531+g0107537.neuron217 requires torch-neuronx>=2.5.0, but you have torch-neuronx 2.1.2.2.2.0 which is incompatible.\n",
      "vllm 0.1.dev4531+g0107537.neuron217 requires transformers>=4.48.2, but you have transformers 4.41.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed neuronx-cc-2.14.227.0+2d4f85be neuronx-distributed-0.8.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 optimum-1.20.0 optimum-neuron-0.0.24 protobuf-3.19.6 tokenizers-0.19.1 torch-2.1.2 torch-neuronx-2.1.2.2.2.0 torch-xla-2.1.3 torchvision-0.16.2 transformers-4.41.1 transformers-neuronx-0.11.351 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: optimum-neuron in /opt/conda/lib/python3.10/site-packages (0.0.24)\n",
      "Collecting optimum-neuron\n",
      "  Using cached optimum_neuron-0.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: neuronx-cc in /opt/conda/lib/python3.10/site-packages (2.14.227.0+2d4f85be)\n",
      "Collecting neuronx-cc\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/neuronx-cc/neuronx_cc-2.17.194.0%2Bd312836f-cp310-cp310-linux_x86_64.whl (589.6 MB)\n",
      "Requirement already satisfied: transformers_neuronx in /opt/conda/lib/python3.10/site-packages (0.11.351)\n",
      "Collecting transformers_neuronx\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/transformers-neuronx/transformers_neuronx-0.13.470-py3-none-any.whl (308 kB)\n",
      "Requirement already satisfied: neuronx_distributed in /opt/conda/lib/python3.10/site-packages (0.8.0)\n",
      "Collecting neuronx_distributed\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/neuronx-distributed/neuronx_distributed-0.11.0-py3-none-linux_x86_64.whl (284 kB)\n",
      "Requirement already satisfied: neuronx-distributed-inference in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch-neuronx in /opt/conda/lib/python3.10/site-packages (2.1.2.2.2.0)\n",
      "Collecting torch-neuronx\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/torch-neuronx/torch_neuronx-2.5.1.2.6.0-py3-none-any.whl (2.6 MB)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.2)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: diffusers in /opt/conda/lib/python3.10/site-packages (0.33.1)\n",
      "Requirement already satisfied: triton in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Collecting triton\n",
      "  Using cached triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (3.19.6)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: networkx~=2.6 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (2.8.8)\n",
      "Requirement already satisfied: scipy<=1.12.0,>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (1.11.2)\n",
      "Requirement already satisfied: python-daemon>=2.2.4 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (3.1.2)\n",
      "Requirement already satisfied: requests<2.32.0 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (2.31.0)\n",
      "Requirement already satisfied: requests-unixsocket>=0.1.5 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (0.4.1)\n",
      "Requirement already satisfied: psutil>=5.6.7 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (7.0.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (1.25.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (0.5.1)\n",
      "Requirement already satisfied: islpy~=2023.1 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (2023.2.5)\n",
      "Requirement already satisfied: pgzip>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (0.3.5)\n",
      "Requirement already satisfied: ec2-metadata~=2.10 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (2.14.0)\n",
      "Requirement already satisfied: tqdm>=4 in /opt/conda/lib/python3.10/site-packages (from neuronx-cc) (4.67.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting optimum~=1.23.3 (from optimum-neuron)\n",
      "  Using cached optimum-1.23.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from optimum-neuron) (0.30.1)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "INFO: pip is looking at multiple versions of transformers-neuronx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers_neuronx\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/transformers-neuronx/transformers_neuronx-0.13.380-py3-none-any.whl (305 kB)\n",
      "  Using cached https://pip.repos.neuron.amazonaws.com/transformers-neuronx/transformers_neuronx-0.13.322-py3-none-any.whl (305 kB)\n",
      "Requirement already satisfied: torch-xla in /opt/conda/lib/python3.10/site-packages (from neuronx_distributed) (2.1.3)\n",
      "Requirement already satisfied: tenacity in /opt/conda/lib/python3.10/site-packages (from neuronx_distributed) (9.1.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from neuronx-distributed-inference) (0.2.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from neuronx-distributed-inference) (0.16.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from neuronx-distributed-inference) (11.1.0)\n",
      "Collecting blobfile (from neuronx-distributed-inference)\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torch-xla (from neuronx_distributed)\n",
      "  Using cached torch_xla-2.5.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: libneuronxla<2.3,>=2.2 in /opt/conda/lib/python3.10/site-packages (from torch-neuronx) (2.2.1630.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.13.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Collecting triton\n",
      "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torch-xla->neuronx_distributed) (2.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from libneuronxla<2.3,>=2.2->torch-neuronx) (1.37.26)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.10/site-packages (from libneuronxla<2.3,>=2.2->torch-neuronx) (1.37.26)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum~=1.23.3->optimum-neuron) (15.0.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum~=1.23.3->optimum-neuron) (2.19.1)\n",
      "Requirement already satisfied: lockfile>=0.10 in /opt/conda/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc) (0.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<2.32.0->neuronx-cc) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<2.32.0->neuronx-cc) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<2.32.0->neuronx-cc) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<2.32.0->neuronx-cc) (2025.1.31)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile->neuronx-distributed-inference)\n",
      "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=4.9 (from blobfile->neuronx-distributed-inference)\n",
      "  Downloading lxml-5.3.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from neuronx-distributed-inference)\n",
      "  Using cached torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Using cached torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->libneuronxla<2.3,>=2.2->torch-neuronx) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from boto3->libneuronxla<2.3,>=2.2->torch-neuronx) (0.11.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore->libneuronxla<2.3,>=2.2->torch-neuronx) (2.9.0.post0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum~=1.23.3->optimum-neuron) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum~=1.23.3->optimum-neuron) (3.11.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum~=1.23.3->optimum-neuron) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->libneuronxla<2.3,>=2.2->torch-neuronx) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum~=1.23.3->optimum-neuron) (2025.2)\n",
      "Using cached optimum_neuron-0.1.0-py3-none-any.whl (399 kB)\n",
      "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Using cached torch_xla-2.5.1-cp310-cp310-manylinux_2_28_x86_64.whl (90.6 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Using cached optimum-1.23.3-py3-none-any.whl (424 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Using cached torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "Downloading lxml-5.3.2-cp310-cp310-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m206.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, pycryptodomex, protobuf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lxml, torch-xla, nvidia-cusolver-cu12, nvidia-cudnn-cu12, blobfile, torch, tokenizers, neuronx-cc, transformers, torchvision, torch-neuronx, optimum, transformers_neuronx, optimum-neuron, neuronx_distributed\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: torch-xla\n",
      "    Found existing installation: torch-xla 2.1.3\n",
      "    Uninstalling torch-xla-2.1.3:\n",
      "      Successfully uninstalled torch-xla-2.1.3\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: neuronx-cc\n",
      "    Found existing installation: neuronx-cc 2.14.227.0+2d4f85be\n",
      "    Uninstalling neuronx-cc-2.14.227.0+2d4f85be:\n",
      "      Successfully uninstalled neuronx-cc-2.14.227.0+2d4f85be\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.41.1\n",
      "    Uninstalling transformers-4.41.1:\n",
      "      Successfully uninstalled transformers-4.41.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.2\n",
      "    Uninstalling torchvision-0.16.2:\n",
      "      Successfully uninstalled torchvision-0.16.2\n",
      "  Attempting uninstall: torch-neuronx\n",
      "    Found existing installation: torch-neuronx 2.1.2.2.2.0\n",
      "    Uninstalling torch-neuronx-2.1.2.2.2.0:\n",
      "      Successfully uninstalled torch-neuronx-2.1.2.2.2.0\n",
      "  Attempting uninstall: optimum\n",
      "    Found existing installation: optimum 1.20.0\n",
      "    Uninstalling optimum-1.20.0:\n",
      "      Successfully uninstalled optimum-1.20.0\n",
      "  Attempting uninstall: transformers_neuronx\n",
      "    Found existing installation: transformers-neuronx 0.11.351\n",
      "    Uninstalling transformers-neuronx-0.11.351:\n",
      "      Successfully uninstalled transformers-neuronx-0.11.351\n",
      "  Attempting uninstall: optimum-neuron\n",
      "    Found existing installation: optimum-neuron 0.0.24\n",
      "    Uninstalling optimum-neuron-0.0.24:\n",
      "      Successfully uninstalled optimum-neuron-0.0.24\n",
      "  Attempting uninstall: neuronx_distributed\n",
      "    Found existing installation: neuronx-distributed 0.8.0\n",
      "    Uninstalling neuronx-distributed-0.8.0:\n",
      "      Successfully uninstalled neuronx-distributed-0.8.0\n",
      "Successfully installed blobfile-3.0.0 lxml-5.3.2 neuronx-cc-2.17.194.0+d312836f neuronx_distributed-0.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 optimum-1.23.3 optimum-neuron-0.1.0 protobuf-3.20.3 pycryptodomex-3.22.0 tokenizers-0.21.1 torch-2.5.1 torch-neuronx-2.5.1.2.6.0 torch-xla-2.5.1 torchvision-0.20.1 transformers-4.48.3 transformers_neuronx-0.13.322 triton-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m✅ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#  Environment Setup\n",
    "# =========================\n",
    "!pip install --upgrade pip \\\n",
    "&& pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com \\\n",
    "&& pip install --upgrade optimum[neuronx] \\\n",
    "&& pip install --upgrade optimum-neuron neuronx-cc transformers_neuronx neuronx_distributed neuronx-cc==2.* neuronx-distributed-inference transformers torch-neuronx accelerate diffusers triton protobuf \\\n",
    "&& pip install faiss-cpu numpy scikit-learn pandas matplotlib seaborn\n",
    "print(\"✅ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40047d3e-1236-4931-ab3e-2dbba25155ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books_rating.csv already exists locally. Skipping download.\n",
      "✅ Books dataset loaded & saved in books_df.pkl.\n"
     ]
    }
   ],
   "source": [
    "! export BOOKS_DF_DS=\"books_df.pkl\"; export NROWS=\"1000\" \\\n",
    "&& python cell_load_books.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3da583-7783-4a9e-b396-cb787b5eaa81",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: \"meta-llama/Llama-3.1-8B-Instruct\"\n",
      "tensor_parallel_size: 8\n",
      "max_num_seqs: 1\n",
      "max_model_len: 2048\n",
      "override_neuron_config:\n",
      "  skip_warmup: true\n",
      "device: \"neuron\"\n",
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n",
      "INFO 04-18 21:35:02 __init__.py:198] Automatically detected platform neuron.\n",
      "INFO 04-18 21:35:10 config.py:542] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-18 21:35:10 config.py:1403] Defaulting to use uni for distributed inference\n",
      "WARNING 04-18 21:35:10 config.py:678] Async output processing is not supported on the current platform type neuron.\n",
      "INFO 04-18 21:35:10 llm_engine.py:234] Initializing a V0 LLM engine (v0.1.dev4531+g0107537) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={'skip_warmup': True}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[1],\"max_capture_size\":1}, use_cached_outputs=False, \n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/attention/utils.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "WARNING 04-18 21:35:11 neuron.py:52] Pin memory is not supported on Neuron.\n",
      "WARNING 04-18 21:35:11 neuron_model_runner.py:116] On-device sampling is turned on in Neuron by default, only top_k, top_p, and temperature are current supported sampling parameters. To turn off the on-device sampling, please set the environment variable NEURON_ON_DEVICE_SAMPLING_DISABLED=1.\n",
      "WARNING 04-18 21:35:11 _custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "WARNING 04-18 21:35:11 config.py:3444] Current VLLM config is not set.\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/llama/modeling_llama.py:60: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "WARNING:root:NeuronConfig init: Unexpected keyword arguments: {'skip_warmup': True}\n",
      "WARNING 04-18 21:35:11 neuronx_distributed.py:192] Exception: [Errno 2] No such file or directory: 'yahavb/nxd_vllm_8B/neuron_config.json'\n",
      "WARNING 04-18 21:35:11 neuronx_distributed.py:193] Failed to load the model from yahavb/nxd_vllm_8B. Recompiling...\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:01<00:00, 30.49s/it]\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-04-18 21:41:39.101: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 8\n",
      "[2025-04-18 21:41:39.102: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-04-18 21:41:39.102: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-04-18 21:41:39.102: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-04-18 21:41:39.102: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 8\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f5119470a60>, 'Ascending Ring PG Group')>\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:41:39.103: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "INFO:Neuron:Generating 5 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.0662531852722168 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 128])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "Add predicate {{0,+,-1}<i0=[0:128:1]>,+,0}<i1=[0:2048:1]>\n",
      "start lb and ub of  {0,+,-1}<i0=[0:128:1]> is 0 0\n",
      "Add predicate {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]>\n",
      "start lb and ub of  {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]> is 255 255\n",
      "before build_invert_ranges alive full {\n",
      "  0 <= i1=[0:2048:1] <= 2047; alive full {\n",
      "    0 <= i1=[0:2048:1] <= 2047; 1 <= i0=[0:128:1] <= 127; alive leaf\n",
      "  }\n",
      "  256 <= i1=[0:2048:1] <= 2047; alive {\n",
      "    256 <= i1=[0:2048:1] <= 2047; 0 <= i0=[0:128:1] <= 127; alive full leaf\n",
      "  }\n",
      "}\n",
      "generated domains alive full {\n",
      "  0 <= i1=[0:2048:1] <= 255; alive {\n",
      "    0 <= i1=[0:2048:1] <= 255; 0 <= i0=[0:128:1] <= 0; alive leaf\n",
      "  }\n",
      "}\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 128]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 256])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 256]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 1024])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 1024]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 2048])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=1, shape=torch.Size([1, 2048]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Generating 5 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.0626988410949707 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:476: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:334: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/generation/sampling.py:286: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.neuron_config.on_cpu)\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=5, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:158: UserWarning: Received an input tensor that was unused. Tensor will be ignored. (index=6, shape=torch.Size([1]), dtype=torch.int32)\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Started compilation for all HLOs\n",
      "....Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done compilation for the priority HLO\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs\n",
      ".........Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "................Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Finished Compilation for all HLOs\n",
      "..Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...7\n",
      "[2025-04-18 21:45:58.965: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 8\n",
      "[2025-04-18 21:45:58.965: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-04-18 21:45:58.965: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-04-18 21:45:58.965: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-04-18 21:45:58.965: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 8\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f5119470a60>, 'Ascending Ring PG Group')>\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[2025-04-18 21:45:58.968: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: lm_head.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.10.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.11.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.12.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.13.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.9.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: norm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.4.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.5.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.6.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.7.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.8.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: embed_tokens.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.0.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.1.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.2.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.3.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.21.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.22.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.23.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.24.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.25.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.26.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.27.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.28.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.29.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.30.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.31.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.14.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.15.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.16.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.17.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.18.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.input_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.mlp.down_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.mlp.gate_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.mlp.up_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.post_attention_layernorm.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.19.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.self_attn.k_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.self_attn.o_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.self_attn.q_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/application_base.py:347: UserWarning: Found float32 weights in checkpoint: layers.20.self_attn.v_proj.weight. Will convert to torch.bfloat16\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Done Sharding weights in 12.094537871191278\n",
      "INFO:Neuron:Skipping model warmup\n",
      "INFO 04-18 21:46:25 executor_base.py:110] # CPU blocks: 1, # CPU blocks: 0\n",
      "INFO 04-18 21:46:25 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 1.00x\n",
      "INFO 04-18 21:46:25 llm_engine.py:435] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n",
      "Running inference with prompt: 'What is Annapurna Labs?'\n",
      "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o2025-Apr-18 21:46:25.0629 216716:221541 [5] net_plugin.cc:70 CCOM WARN NET/Plugin: Error: libnccom-net.so load failed. libfabric.so.1: cannot open shared object file: No such file or directory. Please make sure to install the latest version of libfabric from https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz.\n",
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 8.35 t\n",
      "Prompt: What is Annapurna Labs?\n",
      "Generated text:  Annapurna Labs is a fabless semiconductor company that specializes in designing and manufacturing System-on-Chip (SoC) solutions for the Internet of Things (IoT) and Artificial Intelligence (AI) markets. The company was founded in 2011 and is headquartered in Sunnyvale, California. Annapurn\n",
      "model.pt: 100%|████████████████████████████| 16.2M/16.2M [00:01<00:00, 13.9MB/s]\n",
      "✅  compilation was successful and stored in yahavb/nxd_vllm_8B!\n"
     ]
    }
   ],
   "source": [
    "# If you haven't compiled the model and pushed to huggingface run the following steps with the models you like to explore. \n",
    "# This example shows the Llama 3.2 1B. Look for more yaml files in the repo and execute the same. \n",
    "! cat nxd_vllm_1b.yaml \\\n",
    "&& export COMPILED_MODEL_ID=\"yahavb/nxd_vllm_1B\" \\\n",
    "&& git config --global credential.helper store \\\n",
    "&& export HUGGINGFACE_TOKEN=hf_ADD_YOUR_HF \\\n",
    "&& python cell_compile_vllm.py nxd_vllm_1b.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6de30783-1ac3-4d9d-9e28-730c745e1564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "WARNING:root:Using non SPMD mode. Set spmd_mode=True if the worlkload is SPMD for a faster trace. Tracing in non SPMD mode for large models can run into OOM errors as we compile all ranks\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "[2025-04-18 21:49:21.680: I neuronx_distributed/parallel_layers/parallel_state.py:588] > initializing tensor model parallel with size 2\n",
      "[2025-04-18 21:49:21.680: I neuronx_distributed/parallel_layers/parallel_state.py:589] > initializing pipeline model parallel with size 1\n",
      "[2025-04-18 21:49:21.680: I neuronx_distributed/parallel_layers/parallel_state.py:590] > initializing context model parallel with size 1\n",
      "[2025-04-18 21:49:21.680: I neuronx_distributed/parallel_layers/parallel_state.py:591] > initializing data parallel with size 1\n",
      "[2025-04-18 21:49:21.680: I neuronx_distributed/parallel_layers/parallel_state.py:592] > initializing world size to 2\n",
      "2025-04-18 21:49:21.000682:  222654  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/no-user/neuroncc_compile_workdir/0d5e7b4a-d7c2-4fb1-8422-56226345a948/model.MODULE_8789140437720179366+e30acd3a.hlo_module.pb --output /tmp/no-user/neuroncc_compile_workdir/0d5e7b4a-d7c2-4fb1-8422-56226345a948/model.MODULE_8789140437720179366+e30acd3a.neff --target=trn1 --verbose=35\n",
      ".Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "2025-Apr-18 21:49:23.0573 222654:224398 [1] net_plugin.cc:70 CCOM WARN NET/Plugin: Error: libnccom-net.so load failed. libfabric.so.1: cannot open shared object file: No such file or directory. Please make sure to install the latest version of libfabric from https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz.\n",
      "2025-04-18 21:49:26.000690:  222657  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.17.194.0+d312836f/MODULE_8789140437720179366+e30acd3a/model.neff\n",
      "2025-Apr-18 21:49:26.0725 222657:224597 [0] net_plugin.cc:70 CCOM WARN NET/Plugin: Error: libnccom-net.so load failed. libfabric.so.1: cannot open shared object file: No such file or directory. Please make sure to install the latest version of libfabric from https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz.\n",
      "[2025-04-18 21:49:26.725: I neuronx_distributed/parallel_layers/parallel_state.py:339] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f59f01a2f80>, 'Ascending Ring PG Group')>\n",
      "[2025-04-18 21:49:26.742: I neuronx_distributed/parallel_layers/parallel_state.py:628] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-04-18 21:49:26.742: I neuronx_distributed/parallel_layers/parallel_state.py:629] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-04-18 21:49:26.742: I neuronx_distributed/parallel_layers/parallel_state.py:630] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-04-18 21:49:26.742: I neuronx_distributed/parallel_layers/parallel_state.py:631] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-04-18 21:49:26.742: I neuronx_distributed/parallel_layers/parallel_state.py:632] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-04-18 21:49:26.742: I neuronx_distributed/parallel_layers/parallel_state.py:633] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "........Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      ".Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "Model compiled successfully! Uploading to yahavb/t5-v1_1-base\n",
      "Upload 2 LFS files:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "tp_0.pt:   0%|                                       | 0.00/183M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "tp_1.pt:   0%|                                       | 0.00/183M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:   1%|▎                             | 2.16M/183M [00:00<00:08, 21.6MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:   1%|▏                             | 1.26M/183M [00:00<00:14, 12.2MB/s]\u001b[A\n",
      "tp_0.pt:   4%|█                             | 6.50M/183M [00:00<00:05, 30.5MB/s]\u001b[A\n",
      "tp_0.pt:   5%|█▌                            | 9.42M/183M [00:00<00:07, 24.4MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:   9%|██▋                           | 16.0M/183M [00:00<00:05, 31.5MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:   9%|██▋                           | 16.0M/183M [00:00<00:08, 19.6MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  15%|████▌                         | 27.7M/183M [00:00<00:04, 35.4MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  16%|████▊                         | 29.6M/183M [00:00<00:03, 40.5MB/s]\u001b[A\n",
      "tp_0.pt:  19%|█████▊                        | 35.0M/183M [00:01<00:05, 27.6MB/s]\u001b[A\n",
      "tp_0.pt:  26%|███████▉                      | 48.0M/183M [00:01<00:05, 26.8MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  18%|█████▎                        | 32.0M/183M [00:01<00:11, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  21%|██████▍                       | 39.3M/183M [00:02<00:07, 18.1MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  35%|██████████▌                   | 64.0M/183M [00:02<00:03, 34.2MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  24%|███████                       | 43.2M/183M [00:02<00:07, 18.8MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  42%|████████████▋                 | 77.4M/183M [00:02<00:02, 45.1MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  25%|███████▋                      | 46.6M/183M [00:02<00:06, 20.0MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  46%|█████████████▋                | 83.6M/183M [00:02<00:03, 32.0MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  27%|████████▏                     | 49.7M/183M [00:02<00:08, 15.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  31%|█████████▏                    | 55.9M/183M [00:02<00:05, 21.3MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  53%|███████████████▊              | 96.0M/183M [00:02<00:02, 35.8MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  33%|█████████▊                    | 59.4M/183M [00:02<00:05, 21.3MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  58%|█████████████████▉             | 106M/183M [00:03<00:01, 43.3MB/s]\u001b[A\n",
      "tp_0.pt:  61%|██████████████████▉            | 112M/183M [00:03<00:02, 34.9MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  34%|██████████▎                   | 62.5M/183M [00:03<00:07, 15.6MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  64%|███████████████████▊           | 117M/183M [00:03<00:02, 26.6MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  36%|██████████▋                   | 64.9M/183M [00:03<00:09, 12.2MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  70%|█████████████████████▋         | 128M/183M [00:03<00:01, 30.0MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  44%|█████████████▏                | 80.0M/183M [00:04<00:04, 23.6MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  79%|████████████████████████▍      | 144M/183M [00:04<00:01, 36.7MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  53%|███████████████▊              | 96.0M/183M [00:04<00:02, 32.1MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  84%|██████████████████████████     | 154M/183M [00:04<00:00, 43.4MB/s]\u001b[A\n",
      "tp_0.pt:  87%|███████████████████████████    | 159M/183M [00:04<00:00, 36.5MB/s]\u001b[A\n",
      "tp_0.pt:  90%|███████████████████████████▊   | 164M/183M [00:05<00:01, 17.8MB/s]\u001b[A\n",
      "tp_0.pt: 100%|███████████████████████████████| 183M/183M [00:06<00:00, 30.4MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Upload 2 LFS files:  50%|████████████▌            | 1/2 [00:06<00:06,  6.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  70%|█████████████████████▋         | 128M/183M [00:06<00:02, 19.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  79%|████████████████████████▍      | 144M/183M [00:07<00:01, 22.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  86%|██████████████████████████▊    | 158M/183M [00:07<00:00, 29.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  90%|███████████████████████████▊   | 164M/183M [00:07<00:00, 26.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt: 100%|███████████████████████████████| 183M/183M [00:08<00:00, 22.3MB/s]\u001b[A\u001b[A\n",
      "Upload 2 LFS files: 100%|█████████████████████████| 2/2 [00:08<00:00,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# If you haven't compiled the embedders models and pushed to huggingface run the following steps with the embedders you like to explore.\n",
    "# This example shows how to compile the google/t5-v1_1-base and we used similar cells with google/t5-v1_1-large and google/t5-v1_1-xl\n",
    "!export COMPILED_MODEL_ID=\"yahavb/t5-v1_1-base\" \\\n",
    "&& export MODEL_ID=\"google/t5-v1_1-base\" \\\n",
    "&& export MAX_SEQ_LEN=2048 \\\n",
    "&& export TP_DEGREE=2 \\\n",
    "&& export HUGGINGFACE_TOKEN=hf_ADD_YOUR_HF \\\n",
    "&& git config --global credential.helper store \\\n",
    "&& python cell_compile_t5.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6da7743-e0c0-4528-a982-1f27bffde182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n",
      "INFO 04-18 21:53:50 __init__.py:198] Automatically detected platform neuron.\n",
      "Loaded the dataset books_df.pkl\n",
      "INFO 04-18 21:54:05 config.py:542] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-18 21:54:05 config.py:1403] Defaulting to use uni for distributed inference\n",
      "WARNING 04-18 21:54:05 config.py:678] Async output processing is not supported on the current platform type neuron.\n",
      "INFO 04-18 21:54:05 llm_engine.py:234] Initializing a V0 LLM engine (v0.1.dev4531+g0107537) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={'skip_warmup': True}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[1],\"max_capture_size\":1}, use_cached_outputs=False, \n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/attention/utils.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_model.py:12: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.gqa import GQA, GroupQueryAttention_QKV\n",
      "WARNING 04-18 21:54:06 neuron.py:52] Pin memory is not supported on Neuron.\n",
      "WARNING 04-18 21:54:06 neuron_model_runner.py:116] On-device sampling is turned on in Neuron by default, only top_k, top_p, and temperature are current supported sampling parameters. To turn off the on-device sampling, please set the environment variable NEURON_ON_DEVICE_SAMPLING_DISABLED=1.\n",
      "WARNING 04-18 21:54:06 _custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: Set seed for `privateuseone` device does not take effect, please add API's `_is_in_bad_fork` and `manual_seed_all` to `privateuseone` device module.\n",
      "  return fn(*args, **kwargs)\n",
      "WARNING 04-18 21:54:06 config.py:3444] Current VLLM config is not set.\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed_inference/models/llama/modeling_llama.py:60: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed_inference.modules.attention.attention_base import NeuronAttentionBase\n",
      "WARNING:root:NeuronConfig init: Unexpected keyword arguments: {'skip_warmup': True}\n",
      "WARNING:root:NeuronConfig init: Unexpected keyword arguments: {'enable_token_tree': False, 'is_prefill_stage': None, 'kv_cache_tiling': False, 'skip_warmup': True, 'weights_to_skip_layout_optimization': []}\n",
      "INFO:Neuron:Skipping model warmup\n",
      "INFO 04-18 21:54:19 neuronx_distributed.py:187] Successfully loaded precompiled model artifacts from yahavb/nxd_vllm_1b\n",
      "INFO 04-18 21:54:19 executor_base.py:110] # CPU blocks: 1, # CPU blocks: 0\n",
      "INFO 04-18 21:54:19 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 1.00x\n",
      "INFO 04-18 21:54:19 llm_engine.py:435] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n",
      "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o2025-Apr-18 21:54:19.0187 225412:225854 [1] net_plugin.cc:70 CCOM WARN NET/Plugin: Error: libnccom-net.so load failed. libfabric.so.1: cannot open shared object file: No such file or directory. Please make sure to install the latest version of libfabric from https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz.\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.61it/s, est. speed input: 47.05 \n",
      "✅ Expanded Interest Example: Also, I'm fascinated by speculative fiction that explores mental health and mental illness,\n",
      "Expanding interests:   0%|                              | 0/125 [00:00<?, ?it/s]⏳ Batch 0-8: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  7.08it/s, est. speed input: 825.39\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.03it/s, est. speed input: 735.91\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.83it/s, est. speed input: 649.89\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.35it/s, est. speed input: 743.35\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.84it/s, est. speed input: 711.63\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.73it/s, est. speed input: 693.85\u001b[A\n",
      "✅ Batch 0-8 complete in 1.40s\n",
      "Expanding interests:   1%|▏                     | 1/125 [00:01<02:53,  1.40s/it]⏳ Batch 8-16: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.86it/s, est. speed input: 952.29\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.56it/s, est. speed input: 775.22\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.79it/s, est. speed input: 644.69\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.39it/s, est. speed input: 608.27\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.19it/s, est. speed input: 606.68\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.07it/s, est. speed input: 596.05\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.43it/s, est. speed input: 600.66\u001b[A\n",
      "✅ Batch 8-16 complete in 1.48s\n",
      "Expanding interests:   2%|▎                     | 2/125 [00:02<02:57,  1.44s/it]⏳ Batch 16-24: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:00,  9.39it/s, est. speed input: 1182.7\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  6.05it/s, est. speed input: 661.91\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.41it/s, est. speed input: 637.42\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.15it/s, est. speed input: 622.03\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 569.42\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.95it/s, est. speed input: 574.83\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.91it/s, est. speed input: 576.74\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.14it/s, est. speed input: 557.93\u001b[A\n",
      "✅ Batch 16-24 complete in 1.56s\n",
      "Expanding interests:   2%|▌                     | 3/125 [00:04<03:02,  1.50s/it]⏳ Batch 24-32: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.85it/s, est. speed input: 841.26\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.54it/s, est. speed input: 683.22\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.80it/s, est. speed input: 537.49\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.42it/s, est. speed input: 539.59\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.22it/s, est. speed input: 544.37\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.09it/s, est. speed input: 547.60\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.45it/s, est. speed input: 521.32\u001b[A\n",
      "✅ Batch 24-32 complete in 1.47s\n",
      "Expanding interests:   3%|▋                     | 4/125 [00:05<02:59,  1.49s/it]⏳ Batch 32-40: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.89it/s, est. speed input: 547.18\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  9.33it/s, est. speed input: 851.85\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  8.50it/s, est. speed input: 802.37\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:00<00:00,  7.18it/s, est. speed input: 756.41\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.04it/s, est. speed input: 719.01\u001b[A\n",
      "✅ Batch 32-40 complete in 1.14s\n",
      "Expanding interests:   4%|▉                     | 5/125 [00:07<02:43,  1.36s/it]⏳ Batch 40-48: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.86it/s, est. speed input: 568.09\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.88it/s, est. speed input: 433.87\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.87it/s, est. speed input: 486.76\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.56it/s, est. speed input: 536.75\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.94it/s, est. speed input: 543.31\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.57it/s, est. speed input: 533.47\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.48it/s, est. speed input: 525.30\u001b[A\n",
      "✅ Batch 40-48 complete in 1.46s\n",
      "Expanding interests:   5%|█                     | 6/125 [00:08<02:46,  1.40s/it]⏳ Batch 48-56: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00, 15.19it/s, est. speed input: 1086.3\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.46it/s, est. speed input: 438.25\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.45it/s, est. speed input: 395.49\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  5.87it/s, est. speed input: 349.70\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.87it/s, est. speed input: 438.79\u001b[A\n",
      "✅ Batch 48-56 complete in 1.02s\n",
      "Expanding interests:   6%|█▏                    | 7/125 [00:09<02:30,  1.27s/it]⏳ Batch 56-64: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.85it/s, est. speed input: 577.50\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.83it/s, est. speed input: 573.24\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.78it/s, est. speed input: 731.00\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.03it/s, est. speed input: 684.28\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.61it/s, est. speed input: 656.47\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.34it/s, est. speed input: 634.64\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.45it/s, est. speed input: 627.21\u001b[A\n",
      "✅ Batch 56-64 complete in 1.47s\n",
      "Expanding interests:   6%|█▍                    | 8/125 [00:11<02:36,  1.34s/it]⏳ Batch 64-72: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.80it/s, est. speed input: 547.79\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.82it/s, est. speed input: 553.93\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.81it/s, est. speed input: 556.36\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.81it/s, est. speed input: 561.85\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  4.82it/s, est. speed input: 475.99\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.83it/s, est. speed input: 448.61\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.85it/s, est. speed input: 467.84\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  4.83it/s, est. speed input: 484.36\u001b[A\n",
      "✅ Batch 64-72 complete in 1.66s\n",
      "Expanding interests:   7%|█▌                    | 9/125 [00:12<02:46,  1.44s/it]⏳ Batch 72-80: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.78it/s, est. speed input: 626.30\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.81it/s, est. speed input: 439.90\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.80it/s, est. speed input: 481.21\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.01it/s, est. speed input: 516.87\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.58it/s, est. speed input: 520.23\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.26it/s, est. speed input: 545.72\u001b[A\n",
      "✅ Batch 72-80 complete in 1.28s\n",
      "Expanding interests:   8%|█▋                   | 10/125 [00:13<02:39,  1.39s/it]⏳ Batch 80-88: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.84it/s, est. speed input: 101.63\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.82it/s, est. speed input: 325.48\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.83it/s, est. speed input: 397.81\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.74it/s, est. speed input: 521.06\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.85it/s, est. speed input: 563.48\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.94it/s, est. speed input: 504.79\u001b[A\n",
      "✅ Batch 80-88 complete in 1.35s\n",
      "Expanding interests:   9%|█▊                   | 11/125 [00:15<02:36,  1.38s/it]⏳ Batch 88-96: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.93it/s, est. speed input: 596.74\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.90it/s, est. speed input: 564.25\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.88it/s, est. speed input: 581.80\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.85it/s, est. speed input: 586.73\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.75it/s, est. speed input: 603.23\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.38it/s, est. speed input: 609.14\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.20it/s, est. speed input: 604.34\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.12it/s, est. speed input: 600.65\u001b[A\n",
      "✅ Batch 88-96 complete in 1.56s\n",
      "Expanding interests:  10%|██                   | 12/125 [00:16<02:42,  1.43s/it]⏳ Batch 96-104: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.09it/s, est. speed input: 679.91\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.30it/s, est. speed input: 645.72\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.66it/s, est. speed input: 635.86\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.35it/s, est. speed input: 616.54\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.17it/s, est. speed input: 622.33\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.06it/s, est. speed input: 568.98\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.38it/s, est. speed input: 577.92\u001b[A\n",
      "✅ Batch 96-104 complete in 1.49s\n",
      "Expanding interests:  10%|██▏                  | 13/125 [00:18<02:42,  1.45s/it]⏳ Batch 104-112: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.85it/s, est. speed input: 218.21\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.84it/s, est. speed input: 394.59\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.82it/s, est. speed input: 471.39\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.84it/s, est. speed input: 469.50\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  4.83it/s, est. speed input: 452.45\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.82it/s, est. speed input: 428.96\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.46it/s, est. speed input: 428.23\u001b[A\n",
      "✅ Batch 104-112 complete in 1.47s\n",
      "Expanding interests:  11%|██▎                  | 14/125 [00:19<02:41,  1.46s/it]⏳ Batch 112-120: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00, 12.64it/s, est. speed input: 939.91\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.10it/s, est. speed input: 598.53\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.31it/s, est. speed input: 609.54\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.84it/s, est. speed input: 589.84\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.31it/s, est. speed input: 573.33\u001b[A\n",
      "✅ Batch 112-120 complete in 1.27s\n",
      "Expanding interests:  12%|██▌                  | 15/125 [00:21<02:34,  1.40s/it]⏳ Batch 120-128: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.84it/s, est. speed input: 527.29\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.83it/s, est. speed input: 345.21\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.85it/s, est. speed input: 378.05\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.56it/s, est. speed input: 513.18\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.85it/s, est. speed input: 535.22\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.11it/s, est. speed input: 482.07\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.77it/s, est. speed input: 490.72\u001b[A\n",
      "✅ Batch 120-128 complete in 1.39s\n",
      "Expanding interests:  13%|██▋                  | 16/125 [00:22<02:32,  1.40s/it]⏳ Batch 128-136: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.83it/s, est. speed input: 608.28\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  9.65it/s, est. speed input: 906.90\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.55it/s, est. speed input: 685.16\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.51it/s, est. speed input: 647.02\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.91it/s, est. speed input: 633.20\u001b[A\n",
      "✅ Batch 128-136 complete in 1.16s\n",
      "Expanding interests:  14%|██▊                  | 17/125 [00:23<02:23,  1.33s/it]⏳ Batch 136-144: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.85it/s, est. speed input: 237.56\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.85it/s, est. speed input: 310.55\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.83it/s, est. speed input: 399.87\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.82it/s, est. speed input: 441.71\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  4.82it/s, est. speed input: 462.26\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.82it/s, est. speed input: 473.80\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.83it/s, est. speed input: 482.10\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  4.83it/s, est. speed input: 493.26\u001b[A\n",
      "✅ Batch 136-144 complete in 1.66s\n",
      "Expanding interests:  14%|███                  | 18/125 [00:25<02:32,  1.43s/it]⏳ Batch 144-152: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.86it/s, est. speed input: 534.95\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.86it/s, est. speed input: 542.17\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.29it/s, est. speed input: 583.72\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.11it/s, est. speed input: 570.59\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 519.77\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.99it/s, est. speed input: 512.37\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.94it/s, est. speed input: 456.80\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  4.97it/s, est. speed input: 463.25\u001b[A\n",
      "✅ Batch 144-152 complete in 1.61s\n",
      "Expanding interests:  15%|███▏                 | 19/125 [00:26<02:37,  1.48s/it]⏳ Batch 152-160: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.81it/s, est. speed input: 557.95\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.35it/s, est. speed input: 609.95\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.26it/s, est. speed input: 567.53\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.73it/s, est. speed input: 476.72\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.95it/s, est. speed input: 461.55\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.30it/s, est. speed input: 464.13\u001b[A\n",
      "✅ Batch 152-160 complete in 1.27s\n",
      "Expanding interests:  16%|███▎                 | 20/125 [00:28<02:28,  1.42s/it]⏳ Batch 160-168: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.83it/s, est. speed input: 154.60\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.82it/s, est. speed input: 373.39\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.82it/s, est. speed input: 424.17\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.81it/s, est. speed input: 453.63\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  4.80it/s, est. speed input: 467.39\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.83it/s, est. speed input: 438.94\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.85it/s, est. speed input: 407.79\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  4.83it/s, est. speed input: 428.11\u001b[A\n",
      "✅ Batch 160-168 complete in 1.66s\n",
      "Expanding interests:  17%|███▌                 | 21/125 [00:29<02:35,  1.49s/it]⏳ Batch 168-176: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.86it/s, est. speed input: 442.48\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.86it/s, est. speed input: 395.79\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.85it/s, est. speed input: 436.83\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.85it/s, est. speed input: 467.21\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  4.87it/s, est. speed input: 470.53\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.86it/s, est. speed input: 430.00\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.84it/s, est. speed input: 436.00\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  4.85it/s, est. speed input: 449.68\u001b[A\n",
      "✅ Batch 168-176 complete in 1.65s\n",
      "Expanding interests:  18%|███▋                 | 22/125 [00:31<02:38,  1.54s/it]⏳ Batch 176-184: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.85it/s, est. speed input: 519.08\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.88it/s, est. speed input: 526.56\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.86it/s, est. speed input: 521.62\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  4.85it/s, est. speed input: 417.41\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  5.05it/s, est. speed input: 440.89\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  4.98it/s, est. speed input: 460.81\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  4.92it/s, est. speed input: 472.15\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  4.92it/s, est. speed input: 450.42\u001b[A\n",
      "✅ Batch 176-184 complete in 1.63s\n",
      "Expanding interests:  18%|███▊                 | 23/125 [00:33<02:39,  1.57s/it]⏳ Batch 184-192: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.85it/s, est. speed input: 237.50\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.82it/s, est. speed input: 205.20\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.81it/s, est. speed input: 322.16\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.03it/s, est. speed input: 358.86\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.61it/s, est. speed input: 317.88\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.28it/s, est. speed input: 321.21\u001b[A\n",
      "✅ Batch 184-192 complete in 1.28s\n",
      "Expanding interests:  19%|████                 | 24/125 [00:34<02:29,  1.48s/it]⏳ Batch 192-200: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.00it/s, est. speed input: 904.70\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  9.00it/s, est. speed input: 794.34\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.17it/s, est. speed input: 586.57\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.26it/s, est. speed input: 581.79\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.32it/s, est. speed input: 636.72\u001b[A\n",
      "✅ Batch 192-200 complete in 1.10s\n",
      "Expanding interests:  20%|████▏                | 25/125 [00:35<02:16,  1.36s/it]⏳ Batch 200-208: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 553.13\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 548.27\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 536.68\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 462.93\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 403.58\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 377.41\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.69it/s, est. speed input: 429.35\u001b[A\n",
      "✅ Batch 200-208 complete in 1.41s\n",
      "Expanding interests:  21%|████▎                | 26/125 [00:36<02:16,  1.38s/it]⏳ Batch 208-216: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 528.90\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 565.44\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 490.97\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 527.47\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 466.46\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 480.89\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 500.13\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 515.29\u001b[A\n",
      "✅ Batch 208-216 complete in 1.60s\n",
      "Expanding interests:  22%|████▌                | 27/125 [00:38<02:21,  1.44s/it]⏳ Batch 216-224: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 115.81\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.58it/s, est. speed input: 656.59\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.43it/s, est. speed input: 649.77\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.88it/s, est. speed input: 645.03\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.57it/s, est. speed input: 577.56\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.39it/s, est. speed input: 531.50\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 540.08\u001b[A\n",
      "✅ Batch 216-224 complete in 1.42s\n",
      "Expanding interests:  22%|████▋                | 28/125 [00:39<02:19,  1.44s/it]⏳ Batch 224-232: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 507.57\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 445.21\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.04it/s, est. speed input: 382.58\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 353.49\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 397.20\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 364.55\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 394.34\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 420.36\u001b[A\n",
      "✅ Batch 224-232 complete in 1.59s\n",
      "Expanding interests:  23%|████▊                | 29/125 [00:41<02:22,  1.48s/it]⏳ Batch 232-240: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.32it/s, est. speed input: 717.82\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.87it/s, est. speed input: 657.71\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.41it/s, est. speed input: 754.42\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:00<00:00,  8.14it/s, est. speed input: 820.34\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.41it/s, est. speed input: 785.48\u001b[A\n",
      "✅ Batch 232-240 complete in 1.08s\n",
      "Expanding interests:  24%|█████                | 30/125 [00:42<02:09,  1.36s/it]⏳ Batch 240-248: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 514.01\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 552.57\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 547.66\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 506.20\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 530.33\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 547.80\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.67it/s, est. speed input: 575.88\u001b[A\n",
      "✅ Batch 240-248 complete in 1.41s\n",
      "Expanding interests:  25%|█████▏               | 31/125 [00:43<02:09,  1.38s/it]⏳ Batch 248-256: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 582.55\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 548.26\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 568.04\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 544.34\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 553.04\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 561.00\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 564.77\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 572.41\u001b[A\n",
      "✅ Batch 248-256 complete in 1.60s\n",
      "Expanding interests:  26%|█████▍               | 32/125 [00:45<02:14,  1.44s/it]⏳ Batch 256-264: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00, 12.05it/s, est. speed input: 1409.9\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.61it/s, est. speed input: 838.10\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.05it/s, est. speed input: 724.82\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  5.71it/s, est. speed input: 621.91\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.49it/s, est. speed input: 552.42\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.89it/s, est. speed input: 498.25\u001b[A\n",
      "✅ Batch 256-264 complete in 1.36s\n",
      "Expanding interests:  26%|█████▌               | 33/125 [00:46<02:10,  1.42s/it]⏳ Batch 264-272: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 321.98\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 564.93\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 539.14\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  8.46it/s, est. speed input: 523.55\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.67it/s, est. speed input: 590.65\u001b[A\n",
      "✅ Batch 264-272 complete in 1.05s\n",
      "Expanding interests:  27%|█████▋               | 34/125 [00:48<01:58,  1.31s/it]⏳ Batch 272-280: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  6.70it/s, est. speed input: 683.51\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.88it/s, est. speed input: 443.92\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.11it/s, est. speed input: 405.88\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.70it/s, est. speed input: 364.28\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.47it/s, est. speed input: 385.21\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.33it/s, est. speed input: 350.75\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.59it/s, est. speed input: 337.82\u001b[A\n",
      "✅ Batch 272-280 complete in 1.43s\n",
      "Expanding interests:  28%|█████▉               | 35/125 [00:49<02:01,  1.34s/it]⏳ Batch 280-288: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.05it/s, est. speed input: 217.06\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.05it/s, est. speed input: 264.92\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.04it/s, est. speed input: 341.30\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.04it/s, est. speed input: 394.34\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 391.88\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  7.85it/s, est. speed input: 507.53\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.12it/s, est. speed input: 464.94\u001b[A\n",
      "✅ Batch 280-288 complete in 1.31s\n",
      "Expanding interests:  29%|██████               | 36/125 [00:50<01:58,  1.33s/it]⏳ Batch 288-296: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.34it/s, est. speed input: 626.07\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.89it/s, est. speed input: 499.58\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.04it/s, est. speed input: 537.33\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.39it/s, est. speed input: 560.37\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.55it/s, est. speed input: 564.48\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.52it/s, est. speed input: 580.54\u001b[A\n",
      "✅ Batch 288-296 complete in 1.23s\n",
      "Expanding interests:  30%|██████▏              | 37/125 [00:51<01:54,  1.30s/it]⏳ Batch 296-304: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 513.83\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.65it/s, est. speed input: 395.36\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.49it/s, est. speed input: 342.06\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.92it/s, est. speed input: 297.54\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.60it/s, est. speed input: 269.74\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.41it/s, est. speed input: 315.56\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.69it/s, est. speed input: 288.78\u001b[A\n",
      "✅ Batch 296-304 complete in 1.41s\n",
      "Expanding interests:  30%|██████▍              | 38/125 [00:53<01:56,  1.33s/it]⏳ Batch 304-312: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 607.78\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 575.18\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 519.72\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 528.14\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 538.82\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 542.90\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 544.55\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 546.95\u001b[A\n",
      "✅ Batch 304-312 complete in 1.59s\n",
      "Expanding interests:  31%|██████▌              | 39/125 [00:54<02:01,  1.41s/it]⏳ Batch 312-320: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 589.13\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 551.04\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 561.57\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 569.76\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 483.08\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.56it/s, est. speed input: 514.20\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 514.04\u001b[A\n",
      "✅ Batch 312-320 complete in 1.41s\n",
      "Expanding interests:  32%|██████▋              | 40/125 [00:56<02:00,  1.41s/it]⏳ Batch 320-328: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.78it/s, est. speed input: 1000.5\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00, 11.23it/s, est. speed input: 1208.0\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:00<00:00,  7.55it/s, est. speed input: 841.90\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.65it/s, est. speed input: 738.47\u001b[A\n",
      "✅ Batch 320-328 complete in 1.05s\n",
      "Expanding interests:  33%|██████▉              | 41/125 [00:57<01:49,  1.30s/it]⏳ Batch 328-336: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 491.65\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 535.95\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.99it/s, est. speed input: 576.15\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.78it/s, est. speed input: 554.78\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  6.16it/s, est. speed input: 506.94\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.77it/s, est. speed input: 519.14\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 491.30\u001b[A\n",
      "✅ Batch 328-336 complete in 1.42s\n",
      "Expanding interests:  34%|███████              | 42/125 [00:58<01:50,  1.34s/it]⏳ Batch 336-344: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 341.89\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 466.99\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 490.32\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 514.49\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 524.31\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 509.08\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 496.00\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 509.07\u001b[A\n",
      "✅ Batch 336-344 complete in 1.60s\n",
      "Expanding interests:  34%|███████▏             | 43/125 [01:00<01:55,  1.41s/it]⏳ Batch 344-352: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 616.42\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 594.08\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.32it/s, est. speed input: 461.99\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.19it/s, est. speed input: 496.39\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.12it/s, est. speed input: 513.59\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.65it/s, est. speed input: 624.92\u001b[A\n",
      "✅ Batch 344-352 complete in 1.21s\n",
      "Expanding interests:  35%|███████▍             | 44/125 [01:01<01:49,  1.35s/it]⏳ Batch 352-360: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 281.87\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.63it/s, est. speed input: 684.61\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.47it/s, est. speed input: 644.75\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:00<00:00,  9.32it/s, est. speed input: 782.10\u001b[A\n",
      "✅ Batch 352-360 complete in 0.86s\n",
      "Expanding interests:  36%|███████▌             | 45/125 [01:02<01:36,  1.20s/it]⏳ Batch 360-368: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 498.35\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 382.63\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 444.68\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 474.42\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 427.76\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 414.44\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 444.60\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 426.65\u001b[A\n",
      "✅ Batch 360-368 complete in 1.59s\n",
      "Expanding interests:  37%|███████▋             | 46/125 [01:04<01:44,  1.32s/it]⏳ Batch 368-376: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 281.06\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 419.46\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 378.61\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 420.92\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 456.06\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 465.54\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 456.54\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 467.96\u001b[A\n",
      "✅ Batch 368-376 complete in 1.60s\n",
      "Expanding interests:  38%|███████▉             | 47/125 [01:05<01:49,  1.40s/it]⏳ Batch 376-384: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 548.74\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  7.07it/s, est. speed input: 643.08\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.97it/s, est. speed input: 609.72\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.56it/s, est. speed input: 543.51\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.35it/s, est. speed input: 473.01\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.24it/s, est. speed input: 407.33\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.05it/s, est. speed input: 431.05\u001b[A\n",
      "✅ Batch 376-384 complete in 1.33s\n",
      "Expanding interests:  38%|████████             | 48/125 [01:07<01:46,  1.38s/it]⏳ Batch 384-392: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 548.24\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 528.06\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 539.55\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 470.23\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 434.56\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 455.17\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 457.68\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 441.27\u001b[A\n",
      "✅ Batch 384-392 complete in 1.59s\n",
      "Expanding interests:  39%|████████▏            | 49/125 [01:08<01:49,  1.44s/it]⏳ Batch 392-400: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.97it/s, est. speed input: 646.25\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 638.24\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.98it/s, est. speed input: 639.26\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.00it/s, est. speed input: 531.68\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  4.99it/s, est. speed input: 564.53\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.00it/s, est. speed input: 562.74\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 540.72\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.00it/s, est. speed input: 510.39\u001b[A\n",
      "✅ Batch 392-400 complete in 1.60s\n",
      "Expanding interests:  40%|████████▍            | 50/125 [01:10<01:51,  1.49s/it]⏳ Batch 400-408: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 161.01\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 231.30\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 363.09\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 366.55\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 415.29\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 448.02\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 474.73\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 490.58\u001b[A\n",
      "✅ Batch 400-408 complete in 1.60s\n",
      "Expanding interests:  41%|████████▌            | 51/125 [01:11<01:52,  1.52s/it]⏳ Batch 408-416: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.97it/s, est. speed input: 640.88\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 611.21\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.98it/s, est. speed input: 625.99\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.00it/s, est. speed input: 600.37\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  5.01it/s, est. speed input: 594.70\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.00it/s, est. speed input: 598.97\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.70it/s, est. speed input: 568.69\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.24it/s, est. speed input: 578.27\u001b[A\n",
      "✅ Batch 408-416 complete in 1.53s\n",
      "Expanding interests:  42%|████████▋            | 52/125 [01:13<01:51,  1.53s/it]⏳ Batch 416-424: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.32it/s, est. speed input: 969.19\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.85it/s, est. speed input: 798.32\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.03it/s, est. speed input: 687.67\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.39it/s, est. speed input: 716.57\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.18it/s, est. speed input: 618.29\u001b[A\n",
      "✅ Batch 416-424 complete in 1.12s\n",
      "Expanding interests:  42%|████████▉            | 53/125 [01:14<01:41,  1.40s/it]⏳ Batch 424-432: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.98it/s, est. speed input: 648.04\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.97it/s, est. speed input: 651.74\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.99it/s, est. speed input: 644.76\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.00it/s, est. speed input: 633.86\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:01<00:00,  5.00it/s, est. speed input: 610.43\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 594.98\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 561.89\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 508.33\u001b[A\n",
      "✅ Batch 424-432 complete in 1.60s\n",
      "Expanding interests:  43%|█████████            | 54/125 [01:16<01:43,  1.46s/it]⏳ Batch 432-440: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.05it/s, est. speed input: 252.57\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.04it/s, est. speed input: 350.54\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.04it/s, est. speed input: 324.11\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 376.27\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 352.32\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 331.14\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 311.14\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 309.30\u001b[A\n",
      "✅ Batch 432-440 complete in 1.59s\n",
      "Expanding interests:  44%|█████████▏           | 55/125 [01:17<01:45,  1.50s/it]⏳ Batch 440-448: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.33it/s, est. speed input: 597.04\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.88it/s, est. speed input: 440.57\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.05it/s, est. speed input: 437.92\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.65it/s, est. speed input: 410.71\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.42it/s, est. speed input: 372.29\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.29it/s, est. speed input: 406.16\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.67it/s, est. speed input: 439.76\u001b[A\n",
      "✅ Batch 440-448 complete in 1.41s\n",
      "Expanding interests:  45%|█████████▍           | 56/125 [01:19<01:41,  1.47s/it]⏳ Batch 448-456: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 591.45\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 454.69\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 464.36\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 460.04\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 443.35\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.53it/s, est. speed input: 636.64\u001b[A\n",
      "✅ Batch 448-456 complete in 1.23s\n",
      "Expanding interests:  46%|█████████▌           | 57/125 [01:20<01:35,  1.40s/it]⏳ Batch 456-464: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  7.53it/s, est. speed input: 809.42\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.21it/s, est. speed input: 721.17\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.70it/s, est. speed input: 677.87\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.43it/s, est. speed input: 669.48\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.28it/s, est. speed input: 657.49\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.24it/s, est. speed input: 752.28\u001b[A\n",
      "✅ Batch 456-464 complete in 1.29s\n",
      "Expanding interests:  46%|█████████▋           | 58/125 [01:21<01:31,  1.37s/it]⏳ Batch 464-472: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 573.53\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 473.23\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.08it/s, est. speed input: 626.38\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.29it/s, est. speed input: 570.09\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.83it/s, est. speed input: 573.35\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.54it/s, est. speed input: 586.87\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 595.42\u001b[A\n",
      "✅ Batch 464-472 complete in 1.42s\n",
      "Expanding interests:  47%|█████████▉           | 59/125 [01:23<01:31,  1.38s/it]⏳ Batch 472-480: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 610.71\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 622.08\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.00it/s, est. speed input: 606.90\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 495.80\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 494.06\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 508.82\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 519.87\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 485.87\u001b[A\n",
      "✅ Batch 472-480 complete in 1.60s\n",
      "Expanding interests:  48%|██████████           | 60/125 [01:24<01:34,  1.45s/it]⏳ Batch 480-488: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 437.94\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00, 10.03it/s, est. speed input: 800.80\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.86it/s, est. speed input: 652.32\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.76it/s, est. speed input: 637.55\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.14it/s, est. speed input: 628.36\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.51it/s, est. speed input: 630.40\u001b[A\n",
      "✅ Batch 480-488 complete in 1.23s\n",
      "Expanding interests:  49%|██████████▏          | 61/125 [01:25<01:28,  1.38s/it]⏳ Batch 488-496: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 597.44\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.63it/s, est. speed input: 667.54\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  8.42it/s, est. speed input: 671.90\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.09it/s, est. speed input: 672.60\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:00<00:00,  7.30it/s, est. speed input: 714.38\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.31it/s, est. speed input: 713.97\u001b[A\n",
      "✅ Batch 488-496 complete in 1.10s\n",
      "Expanding interests:  50%|██████████▍          | 62/125 [01:26<01:21,  1.30s/it]⏳ Batch 496-504: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 215.74\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 364.01\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 454.34\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.00it/s, est. speed input: 497.08\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.00it/s, est. speed input: 529.26\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 483.11\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 476.62\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 451.73\u001b[A\n",
      "✅ Batch 496-504 complete in 1.60s\n",
      "Expanding interests:  50%|██████████▌          | 63/125 [01:28<01:25,  1.39s/it]⏳ Batch 504-512: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.83it/s, est. speed input: 680.06\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.71it/s, est. speed input: 519.49\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.97it/s, est. speed input: 431.04\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.60it/s, est. speed input: 454.25\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.40it/s, est. speed input: 393.33\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.27it/s, est. speed input: 391.51\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.63it/s, est. speed input: 383.90\u001b[A\n",
      "✅ Batch 504-512 complete in 1.42s\n",
      "Expanding interests:  51%|██████████▊          | 64/125 [01:29<01:25,  1.40s/it]⏳ Batch 512-520: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  6.95it/s, est. speed input: 510.96\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.00it/s, est. speed input: 484.99\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.62it/s, est. speed input: 507.72\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.06it/s, est. speed input: 535.55\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.80it/s, est. speed input: 584.92\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.32it/s, est. speed input: 596.17\u001b[A\n",
      "✅ Batch 512-520 complete in 1.27s\n",
      "Expanding interests:  52%|██████████▉          | 65/125 [01:31<01:21,  1.36s/it]⏳ Batch 520-528: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 616.27\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.04it/s, est. speed input: 467.93\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.04it/s, est. speed input: 414.46\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 392.58\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 438.43\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.55it/s, est. speed input: 515.09\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 528.03\u001b[A\n",
      "✅ Batch 520-528 complete in 1.41s\n",
      "Expanding interests:  53%|███████████          | 66/125 [01:32<01:21,  1.37s/it]⏳ Batch 528-536: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 591.61\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 555.04\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 552.65\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 551.50\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 552.67\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 565.44\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 571.53\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 578.45\u001b[A\n",
      "✅ Batch 528-536 complete in 1.60s\n",
      "Expanding interests:  54%|███████████▎         | 67/125 [01:34<01:23,  1.44s/it]⏳ Batch 536-544: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 625.84\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.63it/s, est. speed input: 841.47\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:00<00:00, 11.65it/s, est. speed input: 950.10\u001b[A\n",
      "✅ Batch 536-544 complete in 0.69s\n",
      "Expanding interests:  54%|███████████▍         | 68/125 [01:34<01:09,  1.22s/it]⏳ Batch 544-552: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 573.53\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 585.03\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 497.67\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 439.76\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 466.35\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 483.18\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 497.91\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.27it/s, est. speed input: 532.52\u001b[A\n",
      "✅ Batch 544-552 complete in 1.52s\n",
      "Expanding interests:  55%|███████████▌         | 69/125 [01:36<01:13,  1.31s/it]⏳ Batch 552-560: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 602.68\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 445.22\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 362.09\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 416.03\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 446.59\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 481.22\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.34it/s, est. speed input: 454.64\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.14it/s, est. speed input: 467.32\u001b[A\n",
      "✅ Batch 552-560 complete in 1.56s\n",
      "Expanding interests:  56%|███████████▊         | 70/125 [01:38<01:16,  1.38s/it]⏳ Batch 560-568: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 201.07\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 344.43\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 279.98\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 369.03\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 415.60\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 400.84\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.53it/s, est. speed input: 462.53\u001b[A\n",
      "✅ Batch 560-568 complete in 1.45s\n",
      "Expanding interests:  57%|███████████▉         | 71/125 [01:39<01:15,  1.40s/it]⏳ Batch 568-576: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.00it/s, est. speed input: 620.61\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 443.83\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 507.48\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 430.00\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 379.34\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 378.95\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 405.98\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 411.89\u001b[A\n",
      "✅ Batch 568-576 complete in 1.60s\n",
      "Expanding interests:  58%|████████████         | 72/125 [01:41<01:17,  1.46s/it]⏳ Batch 576-584: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 577.71\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 576.99\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.06it/s, est. speed input: 622.87\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.27it/s, est. speed input: 615.58\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.83it/s, est. speed input: 593.54\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.56it/s, est. speed input: 591.62\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.67it/s, est. speed input: 595.18\u001b[A\n",
      "✅ Batch 576-584 complete in 1.41s\n",
      "Expanding interests:  58%|████████████▎        | 73/125 [01:42<01:15,  1.45s/it]⏳ Batch 584-592: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.72it/s, est. speed input: 1103.7\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.67it/s, est. speed input: 867.59\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.65it/s, est. speed input: 909.18\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.67it/s, est. speed input: 828.09\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.21it/s, est. speed input: 849.91\u001b[A\n",
      "✅ Batch 584-592 complete in 1.11s\n",
      "Expanding interests:  59%|████████████▍        | 74/125 [01:43<01:08,  1.35s/it]⏳ Batch 592-600: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 592.17\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 593.68\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 571.62\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 559.79\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 548.28\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 540.78\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 543.93\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 553.62\u001b[A\n",
      "✅ Batch 592-600 complete in 1.60s\n",
      "Expanding interests:  60%|████████████▌        | 75/125 [01:45<01:11,  1.42s/it]⏳ Batch 600-608: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 617.05\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 616.52\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 606.45\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 603.92\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.00it/s, est. speed input: 608.95\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 587.06\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 541.48\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 504.13\u001b[A\n",
      "✅ Batch 600-608 complete in 1.60s\n",
      "Expanding interests:  61%|████████████▊        | 76/125 [01:46<01:12,  1.47s/it]⏳ Batch 608-616: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.05it/s, est. speed input: 166.70\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00, 10.06it/s, est. speed input: 430.74\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.86it/s, est. speed input: 344.95\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.78it/s, est. speed input: 322.48\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.16it/s, est. speed input: 316.39\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.54it/s, est. speed input: 311.75\u001b[A\n",
      "✅ Batch 608-616 complete in 1.22s\n",
      "Expanding interests:  62%|████████████▉        | 77/125 [01:47<01:07,  1.40s/it]⏳ Batch 616-624: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.96it/s, est. speed input: 649.92\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 640.71\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.00it/s, est. speed input: 621.31\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 608.26\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 544.09\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 527.13\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 474.23\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 488.87\u001b[A\n",
      "✅ Batch 616-624 complete in 1.60s\n",
      "Expanding interests:  62%|█████████████        | 78/125 [01:49<01:08,  1.46s/it]⏳ Batch 624-632: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 438.06\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 504.96\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 494.19\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.80it/s, est. speed input: 618.90\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  6.18it/s, est. speed input: 563.32\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.78it/s, est. speed input: 559.66\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 515.08\u001b[A\n",
      "✅ Batch 624-632 complete in 1.41s\n",
      "Expanding interests:  63%|█████████████▎       | 79/125 [01:51<01:06,  1.44s/it]⏳ Batch 632-640: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 80.60 \u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 226.44\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.07it/s, est. speed input: 401.78\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.28it/s, est. speed input: 355.16\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.84it/s, est. speed input: 374.34\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.56it/s, est. speed input: 410.19\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 435.12\u001b[A\n",
      "✅ Batch 632-640 complete in 1.41s\n",
      "Expanding interests:  64%|█████████████▍       | 80/125 [01:52<01:04,  1.43s/it]⏳ Batch 640-648: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  6.17it/s, est. speed input: 413.32\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.44it/s, est. speed input: 326.69\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.24it/s, est. speed input: 431.66\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.15it/s, est. speed input: 466.20\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.10it/s, est. speed input: 496.25\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.12it/s, est. speed input: 473.54\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.63it/s, est. speed input: 486.11\u001b[A\n",
      "✅ Batch 640-648 complete in 1.42s\n",
      "Expanding interests:  65%|█████████████▌       | 81/125 [01:53<01:02,  1.43s/it]⏳ Batch 648-656: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 533.05\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 571.86\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 567.95\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 575.12\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 572.57\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 570.30\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 574.63\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 572.52\u001b[A\n",
      "✅ Batch 648-656 complete in 1.60s\n",
      "Expanding interests:  66%|█████████████▊       | 82/125 [01:55<01:03,  1.48s/it]⏳ Batch 656-664: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  8.82it/s, est. speed input: 992.18\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.70it/s, est. speed input: 718.12\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  7.34it/s, est. speed input: 757.94\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:00<00:00,  9.46it/s, est. speed input: 896.32\u001b[A\n",
      "✅ Batch 656-664 complete in 0.85s\n",
      "Expanding interests:  66%|█████████████▉       | 83/125 [01:56<00:54,  1.29s/it]⏳ Batch 664-672: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00, 13.09it/s, est. speed input: 1265.4\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  8.90it/s, est. speed input: 978.05\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.42it/s, est. speed input: 782.57\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:00<00:00,  8.37it/s, est. speed input: 850.34\u001b[A\n",
      "✅ Batch 664-672 complete in 0.96s\n",
      "Expanding interests:  67%|██████████████       | 84/125 [01:57<00:48,  1.19s/it]⏳ Batch 672-680: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 90.73 \u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 294.37\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 385.55\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 427.47\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 462.36\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 478.34\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.00it/s, est. speed input: 502.37\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 491.19\u001b[A\n",
      "✅ Batch 672-680 complete in 1.60s\n",
      "Expanding interests:  68%|██████████████▎      | 85/125 [01:58<00:52,  1.31s/it]⏳ Batch 680-688: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 607.15\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 596.77\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.84it/s, est. speed input: 659.85\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.47it/s, est. speed input: 661.73\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.30it/s, est. speed input: 644.81\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.20it/s, est. speed input: 635.51\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.14it/s, est. speed input: 625.96\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.22it/s, est. speed input: 612.88\u001b[A\n",
      "✅ Batch 680-688 complete in 1.54s\n",
      "Expanding interests:  69%|██████████████▍      | 86/125 [02:00<00:53,  1.38s/it]⏳ Batch 688-696: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 160.75\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 246.47\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 360.47\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 339.65\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 339.14\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 368.14\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 401.70\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 426.81\u001b[A\n",
      "✅ Batch 688-696 complete in 1.59s\n",
      "Expanding interests:  70%|██████████████▌      | 87/125 [02:01<00:54,  1.44s/it]⏳ Batch 696-704: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 594.68\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.64it/s, est. speed input: 813.86\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  8.44it/s, est. speed input: 756.53\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.13it/s, est. speed input: 722.62\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  7.68it/s, est. speed input: 715.47\u001b[A\n",
      "✅ Batch 696-704 complete in 1.04s\n",
      "Expanding interests:  70%|██████████████▊      | 88/125 [02:03<00:49,  1.32s/it]⏳ Batch 704-712: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.33it/s, est. speed input: 947.48\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.88it/s, est. speed input: 643.93\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.04it/s, est. speed input: 569.92\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.65it/s, est. speed input: 491.75\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.42it/s, est. speed input: 498.89\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.29it/s, est. speed input: 446.58\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 460.86\u001b[A\n",
      "✅ Batch 704-712 complete in 1.41s\n",
      "Expanding interests:  71%|██████████████▉      | 89/125 [02:04<00:48,  1.35s/it]⏳ Batch 712-720: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.75it/s, est. speed input: 632.18\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.29it/s, est. speed input: 615.88\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.25it/s, est. speed input: 747.59\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:00<00:00,  9.59it/s, est. speed input: 954.16\u001b[A\n",
      "✅ Batch 712-720 complete in 0.84s\n",
      "Expanding interests:  72%|███████████████      | 90/125 [02:05<00:41,  1.20s/it]⏳ Batch 720-728: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.06it/s, est. speed input: 237.73\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.04it/s, est. speed input: 390.53\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  8.42it/s, est. speed input: 741.59\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.96it/s, est. speed input: 709.07\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  6.22it/s, est. speed input: 680.87\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.91it/s, est. speed input: 777.15\u001b[A\n",
      "✅ Batch 720-728 complete in 1.16s\n",
      "Expanding interests:  73%|███████████████▎     | 91/125 [02:06<00:40,  1.19s/it]⏳ Batch 728-736: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 625.87\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.00it/s, est. speed input: 625.34\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 501.11\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 462.62\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 490.53\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.56it/s, est. speed input: 505.44\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 494.61\u001b[A\n",
      "✅ Batch 728-736 complete in 1.41s\n",
      "Expanding interests:  74%|███████████████▍     | 92/125 [02:07<00:41,  1.25s/it]⏳ Batch 736-744: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 427.21\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.98it/s, est. speed input: 543.94\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.04it/s, est. speed input: 518.82\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:00<00:00, 10.49it/s, est. speed input: 709.62\u001b[A\n",
      "✅ Batch 736-744 complete in 0.76s\n",
      "Expanding interests:  74%|███████████████▌     | 93/125 [02:08<00:35,  1.11s/it]⏳ Batch 744-752: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.36it/s, est. speed input: 660.16\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.87it/s, est. speed input: 607.39\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.04it/s, est. speed input: 490.04\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.65it/s, est. speed input: 416.64\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.42it/s, est. speed input: 424.63\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.53it/s, est. speed input: 482.39\u001b[A\n",
      "✅ Batch 744-752 complete in 1.23s\n",
      "Expanding interests:  75%|███████████████▊     | 94/125 [02:09<00:35,  1.14s/it]⏳ Batch 752-760: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.00it/s, est. speed input: 625.30\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 603.22\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 572.97\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 584.58\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 586.84\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 581.95\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.00it/s, est. speed input: 590.41\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.00it/s, est. speed input: 592.93\u001b[A\n",
      "✅ Batch 752-760 complete in 1.60s\n",
      "Expanding interests:  76%|███████████████▉     | 95/125 [02:11<00:38,  1.28s/it]⏳ Batch 760-768: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 595.95\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.63it/s, est. speed input: 521.97\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.46it/s, est. speed input: 544.25\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.89it/s, est. speed input: 556.83\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.57it/s, est. speed input: 572.23\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.39it/s, est. speed input: 537.73\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.67it/s, est. speed input: 543.54\u001b[A\n",
      "✅ Batch 760-768 complete in 1.41s\n",
      "Expanding interests:  77%|████████████████▏    | 96/125 [02:12<00:38,  1.32s/it]⏳ Batch 768-776: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 413.01\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 499.93\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 535.24\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 496.99\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 515.74\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 521.72\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 475.19\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 490.41\u001b[A\n",
      "✅ Batch 768-776 complete in 1.60s\n",
      "Expanding interests:  78%|████████████████▎    | 97/125 [02:14<00:39,  1.40s/it]⏳ Batch 776-784: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.00it/s, est. speed input: 624.51\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.40it/s, est. speed input: 642.53\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.57it/s, est. speed input: 730.76\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.99it/s, est. speed input: 693.03\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.64it/s, est. speed input: 708.06\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.37it/s, est. speed input: 737.84\u001b[A\n",
      "✅ Batch 776-784 complete in 1.26s\n",
      "Expanding interests:  78%|████████████████▍    | 98/125 [02:15<00:36,  1.36s/it]⏳ Batch 784-792: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 627.04\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 599.32\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 607.72\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 503.06\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 526.47\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 538.86\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.11it/s, est. speed input: 555.41\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.05it/s, est. speed input: 564.25\u001b[A\n",
      "✅ Batch 784-792 complete in 1.59s\n",
      "Expanding interests:  79%|████████████████▋    | 99/125 [02:17<00:37,  1.43s/it]⏳ Batch 792-800: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  7.59it/s, est. speed input: 527.55\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.22it/s, est. speed input: 574.23\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.72it/s, est. speed input: 470.12\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.14it/s, est. speed input: 561.52\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.86it/s, est. speed input: 512.23\u001b[A\n",
      "✅ Batch 792-800 complete in 1.17s\n",
      "Expanding interests:  80%|████████████████    | 100/125 [02:18<00:33,  1.35s/it]⏳ Batch 800-808: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 592.09\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 626.57\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 460.55\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 482.14\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 464.24\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.52it/s, est. speed input: 517.32\u001b[A\n",
      "✅ Batch 800-808 complete in 1.23s\n",
      "Expanding interests:  81%|████████████████▏   | 101/125 [02:19<00:31,  1.31s/it]⏳ Batch 808-816: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 601.28\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 603.76\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 604.62\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.00it/s, est. speed input: 605.84\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 603.87\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 598.55\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 669.98\u001b[A\n",
      "✅ Batch 808-816 complete in 1.42s\n",
      "Expanding interests:  82%|████████████████▎   | 102/125 [02:21<00:30,  1.35s/it]⏳ Batch 816-824: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 621.12\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 531.95\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  7.06it/s, est. speed input: 673.64\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  6.28it/s, est. speed input: 637.83\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.84it/s, est. speed input: 579.93\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.56it/s, est. speed input: 581.77\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 577.92\u001b[A\n",
      "✅ Batch 816-824 complete in 1.41s\n",
      "Expanding interests:  82%|████████████████▍   | 103/125 [02:22<00:30,  1.37s/it]⏳ Batch 824-832: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 552.85\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 344.38\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 423.40\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 459.69\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.00it/s, est. speed input: 496.17\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.52it/s, est. speed input: 559.19\u001b[A\n",
      "✅ Batch 824-832 complete in 1.23s\n",
      "Expanding interests:  83%|████████████████▋   | 104/125 [02:23<00:27,  1.32s/it]⏳ Batch 832-840: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  7.59it/s, est. speed input: 459.11\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.25it/s, est. speed input: 509.61\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  8.42it/s, est. speed input: 706.20\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.08it/s, est. speed input: 674.51\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.33it/s, est. speed input: 666.99\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.51it/s, est. speed input: 600.06\u001b[A\n",
      "✅ Batch 832-840 complete in 1.23s\n",
      "Expanding interests:  84%|████████████████▊   | 105/125 [02:24<00:25,  1.30s/it]⏳ Batch 840-848: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 451.60\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 543.54\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  4.99it/s, est. speed input: 577.74\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 570.55\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 575.56\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 579.91\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 570.90\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 555.07\u001b[A\n",
      "✅ Batch 840-848 complete in 1.60s\n",
      "Expanding interests:  85%|████████████████▉   | 106/125 [02:26<00:26,  1.39s/it]⏳ Batch 848-856: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.00it/s, est. speed input: 630.20\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 451.59\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  5.00it/s, est. speed input: 516.91\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 534.38\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 491.84\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 452.72\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 489.61\u001b[A\n",
      "✅ Batch 848-856 complete in 1.42s\n",
      "Expanding interests:  86%|█████████████████   | 107/125 [02:27<00:25,  1.40s/it]⏳ Batch 856-864: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.35it/s, est. speed input: 360.19\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.88it/s, est. speed input: 302.86\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.05it/s, est. speed input: 268.13\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.65it/s, est. speed input: 275.00\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.43it/s, est. speed input: 249.68\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.29it/s, est. speed input: 303.56\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 350.66\u001b[A\n",
      "✅ Batch 856-864 complete in 1.41s\n",
      "Expanding interests:  86%|█████████████████▎  | 108/125 [02:29<00:23,  1.40s/it]⏳ Batch 864-872: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 156.21\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.99it/s, est. speed input: 397.62\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 467.30\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 494.57\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 521.84\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 531.12\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 603.22\u001b[A\n",
      "✅ Batch 864-872 complete in 1.42s\n",
      "Expanding interests:  87%|█████████████████▍  | 109/125 [02:30<00:22,  1.41s/it]⏳ Batch 872-880: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 393.08\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 490.12\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 538.46\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 547.31\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 493.17\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.18it/s, est. speed input: 471.49\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.53it/s, est. speed input: 457.91\u001b[A\n",
      "✅ Batch 872-880 complete in 1.45s\n",
      "Expanding interests:  88%|█████████████████▌  | 110/125 [02:32<00:21,  1.42s/it]⏳ Batch 880-888: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 572.86\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 490.03\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  5.00it/s, est. speed input: 548.85\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 456.08\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 489.86\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.42it/s, est. speed input: 542.96\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.03it/s, est. speed input: 520.36\u001b[A\n",
      "✅ Batch 880-888 complete in 1.33s\n",
      "Expanding interests:  89%|█████████████████▊  | 111/125 [02:33<00:19,  1.39s/it]⏳ Batch 888-896: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 201.40\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 413.81\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 343.23\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 373.17\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 330.79\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.57it/s, est. speed input: 329.44\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.69it/s, est. speed input: 309.92\u001b[A\n",
      "✅ Batch 888-896 complete in 1.41s\n",
      "Expanding interests:  90%|█████████████████▉  | 112/125 [02:35<00:18,  1.40s/it]⏳ Batch 896-904: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 372.30\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 293.96\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 375.28\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.03it/s, est. speed input: 371.99\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 351.90\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 400.64\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.67it/s, est. speed input: 490.39\u001b[A\n",
      "✅ Batch 896-904 complete in 1.41s\n",
      "Expanding interests:  90%|██████████████████  | 113/125 [02:36<00:16,  1.40s/it]⏳ Batch 904-912: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:00,  9.35it/s, est. speed input: 593.88\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  6.88it/s, est. speed input: 513.38\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.03it/s, est. speed input: 538.44\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.65it/s, est. speed input: 493.13\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.43it/s, est. speed input: 419.88\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.30it/s, est. speed input: 378.18\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.68it/s, est. speed input: 408.51\u001b[A\n",
      "✅ Batch 904-912 complete in 1.41s\n",
      "Expanding interests:  91%|██████████████████▏ | 114/125 [02:37<00:15,  1.40s/it]⏳ Batch 912-920: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 621.30\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 584.74\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  8.98it/s, est. speed input: 588.97\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:00<00:00,  7.49it/s, est. speed input: 480.10\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  6.61it/s, est. speed input: 505.31\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  6.52it/s, est. speed input: 447.72\u001b[A\n",
      "✅ Batch 912-920 complete in 1.23s\n",
      "Expanding interests:  92%|██████████████████▍ | 115/125 [02:39<00:13,  1.35s/it]⏳ Batch 920-928: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 611.59\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 456.98\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 447.13\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 449.57\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.94it/s, est. speed input: 529.33\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.57it/s, est. speed input: 554.14\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.38it/s, est. speed input: 501.43\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.30it/s, est. speed input: 479.06\u001b[A\n",
      "✅ Batch 920-928 complete in 1.51s\n",
      "Expanding interests:  93%|██████████████████▌ | 116/125 [02:40<00:12,  1.40s/it]⏳ Batch 928-936: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.02it/s, est. speed input: 230.90\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 201.01\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 232.68\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 224.78\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 229.15\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 221.97\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 215.39\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 213.04\u001b[A\n",
      "✅ Batch 928-936 complete in 1.59s\n",
      "Expanding interests:  94%|██████████████████▋ | 117/125 [02:42<00:11,  1.46s/it]⏳ Batch 936-944: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.04it/s, est. speed input: 543.89\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.02it/s, est. speed input: 587.38\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 591.81\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.01it/s, est. speed input: 598.82\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.01it/s, est. speed input: 598.52\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 599.57\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 600.41\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 601.12\u001b[A\n",
      "✅ Batch 936-944 complete in 1.60s\n",
      "Expanding interests:  94%|██████████████████▉ | 118/125 [02:43<00:10,  1.50s/it]⏳ Batch 944-952: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 606.27\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.00it/s, est. speed input: 618.03\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.01it/s, est. speed input: 510.88\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 451.26\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.00it/s, est. speed input: 488.35\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 421.53\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 440.87\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 464.61\u001b[A\n",
      "✅ Batch 944-952 complete in 1.60s\n",
      "Expanding interests:  95%|███████████████████ | 119/125 [02:45<00:09,  1.53s/it]⏳ Batch 952-960: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.00it/s, est. speed input: 614.78\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 575.54\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 546.34\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 545.64\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 465.96\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 438.74\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 419.20\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 442.09\u001b[A\n",
      "✅ Batch 952-960 complete in 1.60s\n",
      "Expanding interests:  96%|███████████████████▏| 120/125 [02:46<00:07,  1.55s/it]⏳ Batch 960-968: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.01it/s, est. speed input: 615.92\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  4.98it/s, est. speed input: 628.31\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:01,  5.00it/s, est. speed input: 611.13\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.00it/s, est. speed input: 612.39\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.00it/s, est. speed input: 609.13\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 610.45\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.01it/s, est. speed input: 561.46\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.01it/s, est. speed input: 541.73\u001b[A\n",
      "✅ Batch 960-968 complete in 1.60s\n",
      "Expanding interests:  97%|███████████████████▎| 121/125 [02:48<00:06,  1.56s/it]⏳ Batch 968-976: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  4.97it/s, est. speed input: 636.17\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 407.85\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 332.53\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 290.86\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.02it/s, est. speed input: 302.92\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.02it/s, est. speed input: 354.36\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.67it/s, est. speed input: 420.73\u001b[A\n",
      "✅ Batch 968-976 complete in 1.41s\n",
      "Expanding interests:  98%|███████████████████▌| 122/125 [02:49<00:04,  1.52s/it]⏳ Batch 976-984: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 362.02\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  7.64it/s, est. speed input: 440.57\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  6.47it/s, est. speed input: 494.84\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.90it/s, est. speed input: 519.85\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.58it/s, est. speed input: 537.73\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.37it/s, est. speed input: 556.55\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.66it/s, est. speed input: 564.29\u001b[A\n",
      "✅ Batch 976-984 complete in 1.42s\n",
      "Expanding interests:  98%|███████████████████▋| 123/125 [02:51<00:02,  1.49s/it]⏳ Batch 984-992: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 568.56\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.01it/s, est. speed input: 601.73\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.02it/s, est. speed input: 586.94\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 584.44\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.00it/s, est. speed input: 596.61\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.01it/s, est. speed input: 534.39\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.02it/s, est. speed input: 529.28\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.02it/s, est. speed input: 536.77\u001b[A\n",
      "✅ Batch 984-992 complete in 1.60s\n",
      "Expanding interests:  99%|███████████████████▊| 124/125 [02:52<00:01,  1.52s/it]⏳ Batch 992-1000: sending 8 prompts\n",
      "\n",
      "Processed prompts:   0%| | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o\u001b[A\n",
      "Processed prompts:  12%|▏| 1/8 [00:00<00:01,  5.03it/s, est. speed input: 276.56\u001b[A\n",
      "Processed prompts:  25%|▎| 2/8 [00:00<00:01,  5.03it/s, est. speed input: 337.01\u001b[A\n",
      "Processed prompts:  38%|▍| 3/8 [00:00<00:00,  5.03it/s, est. speed input: 417.36\u001b[A\n",
      "Processed prompts:  50%|▌| 4/8 [00:00<00:00,  5.02it/s, est. speed input: 471.02\u001b[A\n",
      "Processed prompts:  62%|▋| 5/8 [00:00<00:00,  5.03it/s, est. speed input: 448.61\u001b[A\n",
      "Processed prompts:  75%|▊| 6/8 [00:01<00:00,  5.03it/s, est. speed input: 467.87\u001b[A\n",
      "Processed prompts:  88%|▉| 7/8 [00:01<00:00,  5.03it/s, est. speed input: 484.13\u001b[A\n",
      "Processed prompts: 100%|█| 8/8 [00:01<00:00,  5.03it/s, est. speed input: 493.69\u001b[A\n",
      "✅ Batch 992-1000 complete in 1.59s\n",
      "Expanding interests: 100%|████████████████████| 125/125 [02:54<00:00,  1.40s/it]\n",
      "✅ Expanded Interests Generated in test_interest_books_nxd_vllm_1b.pkl!\n",
      "\n",
      "📊 LATENCY REPORT for User Interest Expansion\n",
      "Latency P0: 689.08 ms\n",
      "Latency P50: 1415.45 ms\n",
      "Latency P90: 1599.38 ms\n",
      "Latency P95: 1611.09 ms\n",
      "Latency P99: 1659.41 ms\n",
      "Latency P100: 1659.41 ms\n"
     ]
    }
   ],
   "source": [
    "# expand user interests with LLM - Llama 3.2 1B deployed with vLLM using NxDI. \n",
    "# Run this cell with all the LLMs you would like to explore. Here we show expanded interest using Llama 3.2 1B model but we also run\n",
    "# it with Deepseek R1 8B and 70B Llama distilled models. \n",
    "! export BOOKS_DF_DS_EXP_INTEREST=\"expanded_interest_books_nxd_vllm_1b.pkl\" \\\n",
    "&& export HUGGINGFACE_TOKEN=hf_ADD_YOUR_HF \\\n",
    "&& git config --global credential.helper store \\\n",
    "&& export BOOKS_DF_DS=\"books_df.pkl\" \\\n",
    "&& export MODEL_ID=\"yahavb/nxd_vllm_1b\" \\\n",
    "&& python cell_expand_interest_llm.py nxd_vllm_1b.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6830804-e1b2-417e-8bfc-d5ce5b26f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id                           Title         User_id  \\\n",
      "0  1882931173  Its Only Art If Its Well Hung!   AVCGYZL8FQQTD   \n",
      "1  0826414346        Dr. Seuss: American Icon  A30TK6U7DNS82R   \n",
      "\n",
      "  review/helpfulness  review/score  review/time  \\\n",
      "0                7/7           4.0    940636800   \n",
      "1              10/10           5.0   1095724800   \n",
      "\n",
      "                           review/summary  \\\n",
      "0  Nice collection of Julie Strain images   \n",
      "1                       Really Enjoyed It   \n",
      "\n",
      "                                         review/text  \\\n",
      "0  This is only for Julie Strain fans. It's a col...   \n",
      "1  I don't care much for Dr. Seuss but after read...   \n",
      "\n",
      "                                   expanded_interest  \\\n",
      "0                                                      \n",
      "1  ads while being inside a public library buildi...   \n",
      "\n",
      "                                        t5_embedding  \n",
      "0  [-0.006195068359375, 0.08447265625, -0.0810546...  \n",
      "1  [-0.0111083984375, 0.0380859375, -0.0588378906...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('expanded_interest_books_nxd_vllm_1b.pkl')\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "186aed34-68c4-4eb9-b0cd-3d1e4dd8dbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:MASTER_ADDR environment variable is not set, defaulting to localhost\n",
      "WARNING:root:Found libneuronpjrt.so. Setting PJRT_DEVICE=NEURON.\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "/opt/conda/lib/python3.10/site-packages/neuronx_distributed/modules/moe/expert_mlps.py:11: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from neuronx_distributed.modules.moe.blockwise import (\n",
      "Loaded dataset path: test_interest_books_nxd_vllm_1b.pkl\n",
      "Fetching 32 files:   0%|                                 | 0/32 [00:00<?, ?it/s]\n",
      "tp_0.pt:   0%|                                       | 0.00/183M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "tp_1.pt:   0%|                                       | 0.00/183M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "tp_0.pt:   6%|█▋                            | 10.5M/183M [00:00<00:02, 76.0MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:   6%|█▋                            | 10.5M/183M [00:00<00:02, 77.4MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  11%|███▍                          | 21.0M/183M [00:00<00:01, 84.0MB/s]\u001b[A\n",
      "tp_0.pt:  17%|█████▏                        | 31.5M/183M [00:00<00:01, 85.9MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  11%|███▍                          | 21.0M/183M [00:00<00:02, 73.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  17%|█████▏                        | 31.5M/183M [00:00<00:01, 80.2MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  23%|██████▉                       | 41.9M/183M [00:00<00:02, 64.9MB/s]\u001b[A\n",
      "tp_0.pt:  29%|████████▌                     | 52.4M/183M [00:00<00:01, 71.7MB/s]\u001b[A\n",
      "tp_0.pt:  34%|██████████▎                   | 62.9M/183M [00:00<00:01, 76.7MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  23%|██████▉                       | 41.9M/183M [00:00<00:02, 48.2MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  40%|████████████                  | 73.4M/183M [00:00<00:01, 79.7MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  29%|████████▌                     | 52.4M/183M [00:00<00:02, 51.6MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  46%|█████████████▊                | 83.9M/183M [00:01<00:01, 68.9MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  34%|██████████▎                   | 62.9M/183M [00:01<00:02, 56.1MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  52%|███████████████▍              | 94.4M/183M [00:01<00:01, 74.0MB/s]\u001b[A\n",
      "tp_0.pt:  57%|█████████████████▊             | 105M/183M [00:01<00:01, 76.6MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  40%|████████████                  | 73.4M/183M [00:01<00:02, 43.7MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  63%|███████████████████▌           | 115M/183M [00:01<00:00, 72.8MB/s]\u001b[A\n",
      "tp_0.pt:  69%|█████████████████████▎         | 126M/183M [00:01<00:00, 76.8MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  46%|█████████████▊                | 83.9M/183M [00:01<00:02, 39.2MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  75%|███████████████████████▏       | 136M/183M [00:01<00:00, 65.0MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  52%|███████████████▍              | 94.4M/183M [00:01<00:01, 45.4MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  80%|████████████████████████▉      | 147M/183M [00:02<00:00, 65.7MB/s]\u001b[A\n",
      "tp_0.pt:  86%|██████████████████████████▋    | 157M/183M [00:02<00:00, 71.1MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  57%|█████████████████▊             | 105M/183M [00:02<00:01, 47.9MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  92%|████████████████████████████▍  | 168M/183M [00:02<00:00, 66.1MB/s]\u001b[A\n",
      "\n",
      "tp_1.pt:  63%|███████████████████▌           | 115M/183M [00:02<00:01, 49.5MB/s]\u001b[A\u001b[A\n",
      "tp_0.pt:  98%|██████████████████████████████▏| 178M/183M [00:02<00:00, 67.6MB/s]\u001b[A\n",
      "\n",
      "tp_0.pt: 100%|███████████████████████████████| 183M/183M [00:02<00:00, 69.1MB/s]\u001b[A\u001b[A\n",
      "Fetching 32 files:   3%|▊                        | 1/32 [00:03<01:33,  3.03s/it]\n",
      "\n",
      "tp_1.pt:  75%|███████████████████████▏       | 136M/183M [00:02<00:00, 56.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  80%|████████████████████████▉      | 147M/183M [00:02<00:00, 48.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  86%|██████████████████████████▋    | 157M/183M [00:03<00:00, 55.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt:  92%|████████████████████████████▍  | 168M/183M [00:03<00:00, 53.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "tp_1.pt: 100%|███████████████████████████████| 183M/183M [00:03<00:00, 51.7MB/s]\u001b[A\u001b[A\n",
      "Fetching 32 files: 100%|████████████████████████| 32/32 [00:05<00:00,  6.35it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "✅ Updated .pkl with 't5_embedding' column: test_interest_books_nxd_vllm_1b.pkl\n",
      "✅ T5 Embeddings & FAISS index saved in test_interest_books_nxd_vllm_1b_t5_base_faiss.index\n",
      "✅ First 5 stored vectors (embeddings):\n",
      "[[ 0.03008398  0.06433344 -0.01805039 ... -0.04211758 -0.02244728\n",
      "   0.06803609]\n",
      " [ 0.02368083  0.04277827  0.03941712 ...  0.07058415  0.0313198\n",
      "   0.05500063]\n",
      " [-0.0300058   0.10001934 -0.02758598 ...  0.00216776  0.00230891\n",
      "   0.05194553]\n",
      " [ 0.03262574  0.03867344 -0.00564982 ...  0.01575585  0.01854097\n",
      "   0.04933649]\n",
      " [ 0.03008398  0.06433344 -0.01805039 ... -0.04211758 -0.02244728\n",
      "   0.06803609]]\n"
     ]
    }
   ],
   "source": [
    "## generate T5-based embeddings and FAISS index from the expanded lists. This example uses meta-llama/Llama-3.2-1B and google/t5-v1_1-base\n",
    "## for the T5-based embeddings, the embeddings are being converted to FAISS (Facebook AI Similarity Search) index. \n",
    "## It is a vector search engine that allows fast nearest-neighbor retrieval. It stores T5-based embeddings computed from expanded interests. Each book review is transformed into a vector.\n",
    "## Used to find books closest in meaning to a user's expanded interest.\n",
    "!export BOOKS_DF_DS_EXP_INTEREST=\"expanded_interest_books_nxd_vllm_1b.pkl\" \\\n",
    "&& export BOOKS_DF_FAISS_IDX=\"expanded_interest_books_nxd_vllm_1b_t5_base_faiss.index\" \\\n",
    "&& export MODEL_ID=\"google/t5-v1_1-base\" \\\n",
    "&& export COMPILED_MODEL_ID=\"yahavb/t5-v1_1-base\" \\\n",
    "&& export MAX_SEQ_LEN=2048 \\\n",
    "&& python cell_t5_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad76ae-ac2a-4d67-b42e-55ec31c3cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## By now we created the similarity index with the user expanded interest for Llama 3.2 1B and google/t5-v1_1-base. \n",
    "## To complete the indexes population run expand_interest_generate_faiss_index.sh script that will compile the models and generate \n",
    "## the indexes for DeepSeek R1 8B and 70B with t5-v1_1-large and t5-v1_1-xl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8fa8f1c-6ea3-4da4-afe6-e9c17578ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered FAISS index files:\n",
      "3b_t5_large: expanded_interest_books_nxd_vllm_3b_t5_large_faiss.index\n",
      "8b_t5_large: expanded_interest_books_nxd_vllm_8b_t5_large_faiss.index\n",
      "3b_t5_xl: expanded_interest_books_nxd_vllm_3b_t5_xl_faiss.index\n",
      "70b_t5_base: expanded_interest_books_nxd_vllm_70b_t5_base_faiss.index\n",
      "1b_t5_large: expanded_interest_books_nxd_vllm_1b_t5_large_faiss.index\n",
      "8b_t5_base: expanded_interest_books_nxd_vllm_8b_t5_base_faiss.index\n",
      "70b_t5_xl: expanded_interest_books_nxd_vllm_70b_t5_xl_faiss.index\n",
      "1b_t5_base: expanded_interest_books_nxd_vllm_1b_t5_base_faiss.index\n",
      "3b_t5_base: expanded_interest_books_nxd_vllm_3b_t5_base_faiss.index\n",
      "8b_t5_xl: expanded_interest_books_nxd_vllm_8b_t5_xl_faiss.index\n",
      "1b_t5_xl: expanded_interest_books_nxd_vllm_1b_t5_xl_faiss.index\n",
      "70b_t5_large: expanded_interest_books_nxd_vllm_70b_t5_large_faiss.index\n",
      "\n",
      "Analyzing FAISS indices:\n",
      "\n",
      "Index '3b_t5_large': Dimension = 1024, Total Vectors = 100000\n",
      "Sample query vector from '3b_t5_large' (first 5 dims): [0.00325331 0.00907313 0.04366668 0.03340067 0.04308831]\n",
      "Raw distances for '3b_t5_large': [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Raw indices for '3b_t5_large': [[  0  37  48  69 100 108 123 129 130 137]]\n",
      "    Top-10 average distance = 0.0\n",
      "\n",
      "Index '8b_t5_large': Dimension = 1024, Total Vectors = 100000\n",
      "Sample query vector from '8b_t5_large' (first 5 dims): [ 0.01092364 -0.02033752 -0.01776202 -0.0351688   0.05044414]\n",
      "Raw distances for '8b_t5_large': [[0.         0.5592806  0.5959495  0.6235127  0.63049674 0.630518\n",
      "  0.652583   0.65780616 0.66568696 0.66585517]]\n",
      "Raw indices for '8b_t5_large': [[    0 62623 31589   699   898 78217 67947 69347 38615 49091]]\n",
      "    Top-10 average distance = 0.5681689381599426\n",
      "\n",
      "Index '3b_t5_xl': Dimension = 2048, Total Vectors = 100000\n",
      "Sample query vector from '3b_t5_xl' (first 5 dims): [-0.00158935  0.02167153 -0.02079465 -0.009082    0.00325699]\n",
      "Raw distances for '3b_t5_xl': [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Raw indices for '3b_t5_xl': [[  0  37  48  69 100 108 123 129 130 137]]\n",
      "    Top-10 average distance = 0.0\n",
      "\n",
      "Index '70b_t5_base': Dimension = 768, Total Vectors = 100000\n",
      "Sample query vector from '70b_t5_base' (first 5 dims): [-0.00389981  0.07653617 -0.01951826  0.00286242  0.00059314]\n",
      "Raw distances for '70b_t5_base': [[0.         0.54215455 0.54885745 0.55408764 0.56844056 0.57371056\n",
      "  0.57486904 0.57568216 0.576208   0.5803784 ]]\n",
      "Raw indices for '70b_t5_base': [[    0 58163 69036 46437 88604  2427 68354  9077 83495 82787]]\n",
      "    Top-10 average distance = 0.5094388723373413\n",
      "\n",
      "Index '1b_t5_large': Dimension = 1024, Total Vectors = 100000\n",
      "Sample query vector from '1b_t5_large' (first 5 dims): [0.00325331 0.00907313 0.04366668 0.03340067 0.04308831]\n",
      "Raw distances for '1b_t5_large': [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Raw indices for '1b_t5_large': [[ 0  4  8 24 33 43 48 54 58 74]]\n",
      "    Top-10 average distance = 0.0\n",
      "\n",
      "Index '8b_t5_base': Dimension = 768, Total Vectors = 100000\n",
      "Sample query vector from '8b_t5_base' (first 5 dims): [-0.02284686  0.06820948 -0.01663848 -0.02930359 -0.01597625]\n",
      "Raw distances for '8b_t5_base': [[0.         0.52034056 0.5317741  0.5437486  0.5474682  0.5476576\n",
      "  0.54786813 0.548418   0.54872364 0.5528761 ]]\n",
      "Raw indices for '8b_t5_base': [[    0 87798   903 16722 64356 40353 24963 79698 70705 73206]]\n",
      "    Top-10 average distance = 0.4888874888420105\n",
      "\n",
      "Index '70b_t5_xl': Dimension = 2048, Total Vectors = 100000\n",
      "Sample query vector from '70b_t5_xl' (first 5 dims): [ 0.02096086 -0.01539539 -0.02544215 -0.04307818 -0.02211732]\n",
      "Raw distances for '70b_t5_xl': [[0.         0.6658592  0.66921043 0.673786   0.674707   0.68497425\n",
      "  0.69060785 0.7047897  0.70673    0.7129022 ]]\n",
      "Raw indices for '70b_t5_xl': [[    0 26758 62549  8106 81899 71517 47882 98460 45510 54511]]\n",
      "    Top-10 average distance = 0.6183565855026245\n",
      "\n",
      "Index '1b_t5_base': Dimension = 768, Total Vectors = 100000\n",
      "Sample query vector from '1b_t5_base' (first 5 dims): [ 0.03008398  0.06433344 -0.01805039  0.00572753 -0.01041369]\n",
      "Raw distances for '1b_t5_base': [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Raw indices for '1b_t5_base': [[ 0  4  8 24 33 43 48 54 58 74]]\n",
      "    Top-10 average distance = 0.0\n",
      "\n",
      "Index '3b_t5_base': Dimension = 768, Total Vectors = 100000\n",
      "Sample query vector from '3b_t5_base' (first 5 dims): [ 0.03008398  0.06433344 -0.01805039  0.00572753 -0.01041369]\n",
      "Raw distances for '3b_t5_base': [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Raw indices for '3b_t5_base': [[  0  37  48  69 100 108 123 129 130 137]]\n",
      "    Top-10 average distance = 0.0\n",
      "\n",
      "Index '8b_t5_xl': Dimension = 2048, Total Vectors = 100000\n",
      "Sample query vector from '8b_t5_xl' (first 5 dims): [-0.02316263 -0.00764623 -0.03224853 -0.03276041  0.03327229]\n",
      "Raw distances for '8b_t5_xl': [[0.         0.76274204 0.7801247  0.8038713  0.8108586  0.81727546\n",
      "  0.83068067 0.8321035  0.8321477  0.8351579 ]]\n",
      "Raw indices for '8b_t5_xl': [[    0 90160 16696 68447 33356 31020 22335 17553 31082 63223]]\n",
      "    Top-10 average distance = 0.7304961681365967\n",
      "\n",
      "Index '1b_t5_xl': Dimension = 2048, Total Vectors = 100000\n",
      "Sample query vector from '1b_t5_xl' (first 5 dims): [-0.00158935  0.02167153 -0.02079465 -0.009082    0.00325699]\n",
      "Raw distances for '1b_t5_xl': [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Raw indices for '1b_t5_xl': [[ 0  4  8 24 33 43 48 54 58 74]]\n",
      "    Top-10 average distance = 0.0\n",
      "\n",
      "Index '70b_t5_large': Dimension = 1024, Total Vectors = 100000\n",
      "Sample query vector from '70b_t5_large' (first 5 dims): [-0.04273473 -0.00735074  0.02355889  0.00426891  0.01018146]\n",
      "Raw distances for '70b_t5_large': [[0.         0.516985   0.5595071  0.5652822  0.5654299  0.5741473\n",
      "  0.5745591  0.57782394 0.5814612  0.58515626]]\n",
      "Raw indices for '70b_t5_large': [[    0 71486 85551 58333 67884  4297  3925 29943 13808 79581]]\n",
      "    Top-10 average distance = 0.5100352168083191\n",
      "\n",
      "Computed average distances:\n",
      "3b_t5_large: 0.0\n",
      "8b_t5_large: 0.5681689381599426\n",
      "3b_t5_xl: 0.0\n",
      "70b_t5_base: 0.5094388723373413\n",
      "1b_t5_large: 0.0\n",
      "8b_t5_base: 0.4888874888420105\n",
      "70b_t5_xl: 0.6183565855026245\n",
      "1b_t5_base: 0.0\n",
      "3b_t5_base: 0.0\n",
      "8b_t5_xl: 0.7304961681365967\n",
      "1b_t5_xl: 0.0\n",
      "70b_t5_large: 0.5100352168083191\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Discover all .index files in the current directory\n",
    "index_files = {}\n",
    "for filepath in glob.glob(\"*.index\"):\n",
    "    base = os.path.basename(filepath)\n",
    "    # Remove common prefixes; adjust if necessary based on your filename conventions.\n",
    "    if base.startswith(\"expanded_interest_books_nxd_vlm_\"):\n",
    "         key = base[len(\"expanded_interest_books_nxd_vlm_\"):]\n",
    "    elif base.startswith(\"expanded_interest_books_nxd_vllm_\"):\n",
    "         key = base[len(\"expanded_interest_books_nxd_vllm_\"):]\n",
    "    else:\n",
    "         key = base\n",
    "    # Remove the trailing suffix \"_faiss.index\"\n",
    "    if key.endswith(\"_faiss.index\"):\n",
    "         key = key[:-len(\"_faiss.index\")]\n",
    "    index_files[key] = filepath\n",
    "\n",
    "if not index_files:\n",
    "    print(\"No FAISS index files discovered in the current directory.\")\n",
    "else:\n",
    "    print(\"Discovered FAISS index files:\")\n",
    "    for k, v in index_files.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "top_k = 10  # Number of neighbors to retrieve\n",
    "\n",
    "avg_dists = {}\n",
    "dimensions = {}\n",
    "\n",
    "print(\"\\nAnalyzing FAISS indices:\\n\")\n",
    "for key, filepath in index_files.items():\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        continue\n",
    "\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(filepath)\n",
    "    dimensions[key] = index.d\n",
    "    print(f\"Index '{key}': Dimension = {index.d}, Total Vectors = {index.ntotal}\")\n",
    "\n",
    "    if index.ntotal == 0:\n",
    "        print(f\"Index '{key}' is empty. Skipping query.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Reconstruct the first vector from the index to use it as a query.\n",
    "    query = np.zeros((1, index.d), dtype=np.float32)\n",
    "    index.reconstruct_n(0, 1, query)\n",
    "\n",
    "    # Print a snippet of the query vector for debugging.\n",
    "    print(f\"Sample query vector from '{key}' (first 5 dims): {query[0][:5]}\")\n",
    "\n",
    "    # Execute a k-NN search using the query vector.\n",
    "    distances, indices = index.search(query, top_k)\n",
    "    print(f\"Raw distances for '{key}': {distances}\")\n",
    "    print(f\"Raw indices for '{key}': {indices}\")\n",
    "\n",
    "    avg_distance = float(distances.mean())\n",
    "    avg_dists[key] = avg_distance\n",
    "    print(f\"    Top-{top_k} average distance = {avg_distance}\\n\")\n",
    "\n",
    "# Debug: Print computed average distances.\n",
    "print(\"Computed average distances:\")\n",
    "for k, v in avg_dists.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Plotting: Create a bar chart comparing the average FAISS distances\n",
    "if not avg_dists:\n",
    "    print(\"No valid average distances computed. Check that your FAISS index files are not empty.\")\n",
    "else:\n",
    "    keys = list(avg_dists.keys())\n",
    "    avg_values = [avg_dists[k] for k in keys]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(keys, avg_values, color='skyblue')\n",
    "    plt.title(\"Average FAISS Distance per LLM-Encoder Combination\")\n",
    "    plt.xlabel(\"LLM-Encoder Combination\")\n",
    "    plt.ylabel(\"Average Distance\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec119314-b148-41d5-b9d7-295921066850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def avg_faiss_distance(llm_variant: str, encoder_variant: str, top_k: int = 5) -> float:\n",
    "    \"\"\"\n",
    "    Load the FAISS index for a given LLM size and encoder size,\n",
    "    reconstruct the 0th vector as a query, and return its mean top-k distance.\n",
    "    \"\"\"\n",
    "    index_path = f\"expanded_interest_books_nxd_vllm_{llm_variant}_{encoder_variant}_faiss.index\"\n",
    "    if not os.path.exists(index_path):\n",
    "        raise FileNotFoundError(f\"Index not found: {index_path}\")\n",
    "    idx = faiss.read_index(index_path)\n",
    "    # reconstruct the first vector\n",
    "    q = np.zeros((1, idx.d), dtype=\"float32\")\n",
    "    idx.reconstruct_n(0, 1, q)\n",
    "    D, _ = idx.search(q, top_k)\n",
    "    return float(D.mean())\n",
    "\n",
    "# Define your experiment grid:\n",
    "llm_variants      = [\"1b\", \"3b\", \"8b\", \"70b\"]\n",
    "encoder_variants  = [\"t5_base\", \"t5_large\", \"t5_xl\"]\n",
    "top_k             = 10\n",
    "\n",
    "# Collect results in a matrix [len(llm) x len(enc)]\n",
    "results = np.zeros((len(llm_variants), len(encoder_variants)), dtype=float)\n",
    "\n",
    "for i, llm in enumerate(llm_variants):\n",
    "    for j, enc in enumerate(encoder_variants):\n",
    "        try:\n",
    "            results[i, j] = avg_faiss_distance(llm, enc, top_k=top_k)\n",
    "        except FileNotFoundError:\n",
    "            results[i, j] = np.nan  # or skip / warn\n",
    "\n",
    "# Now plot a grouped bar chart:\n",
    "x = np.arange(len(llm_variants))  # one tick per LLM variant\n",
    "width = 0.25  # width of each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for j, enc in enumerate(encoder_variants):\n",
    "    ax.bar(x + j*width, results[:, j], width, label=enc.replace(\"t5_\", \"\").upper())\n",
    "\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(llm_variants)\n",
    "ax.set_xlabel(\"LLM Variant\")\n",
    "ax.set_ylabel(f\"Average FAISS Distance (top-{top_k})\")\n",
    "ax.set_title(\"Cold‑Start Expansion: LLM Size vs. Encoder Size\")\n",
    "ax.legend(title=\"Encoder\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613b7e2a-cb95-421c-8f4b-7dff40655e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overlap among encoders for LLM=1b:\n",
      "  t5_base vs t5_large: 3/5 (60.0%)\n",
      "  t5_base vs t5_xl: 3/5 (60.0%)\n",
      "  t5_large vs t5_xl: 3/5 (60.0%)\n",
      "\n",
      "Overlap among encoders for LLM=3b:\n",
      "  t5_base vs t5_large: 5/5 (100.0%)\n",
      "  t5_base vs t5_xl: 5/5 (100.0%)\n",
      "  t5_large vs t5_xl: 5/5 (100.0%)\n",
      "\n",
      "Overlap among encoders for LLM=8b:\n",
      "  t5_base vs t5_large: 2/5 (40.0%)\n",
      "  t5_base vs t5_xl: 2/5 (40.0%)\n",
      "  t5_large vs t5_xl: 1/5 (20.0%)\n",
      "\n",
      "Overlap among encoders for LLM=70b:\n",
      "  t5_base vs t5_large: 2/5 (40.0%)\n",
      "  t5_base vs t5_xl: 1/5 (20.0%)\n",
      "  t5_large vs t5_xl: 1/5 (20.0%)\n",
      "\n",
      "Overlap among LLMs for Encoder=t5_base:\n",
      "  1b vs 3b: 2/5 (40.0%)\n",
      "  1b vs 8b: 1/5 (20.0%)\n",
      "  1b vs 70b: 1/5 (20.0%)\n",
      "  3b vs 8b: 1/5 (20.0%)\n",
      "  3b vs 70b: 1/5 (20.0%)\n",
      "  8b vs 70b: 2/5 (40.0%)\n",
      "\n",
      "Overlap among LLMs for Encoder=t5_large:\n",
      "  1b vs 3b: 2/5 (40.0%)\n",
      "  1b vs 8b: 1/5 (20.0%)\n",
      "  1b vs 70b: 1/5 (20.0%)\n",
      "  3b vs 8b: 1/5 (20.0%)\n",
      "  3b vs 70b: 1/5 (20.0%)\n",
      "  8b vs 70b: 1/5 (20.0%)\n",
      "\n",
      "Overlap among LLMs for Encoder=t5_xl:\n",
      "  1b vs 3b: 2/5 (40.0%)\n",
      "  1b vs 8b: 1/5 (20.0%)\n",
      "  1b vs 70b: 1/5 (20.0%)\n",
      "  3b vs 8b: 1/5 (20.0%)\n",
      "  3b vs 70b: 1/5 (20.0%)\n",
      "  8b vs 70b: 1/5 (20.0%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "def recommend_books(user_reviews, llm_variant, encoder_variant, top_k=5):\n",
    "    \"\"\"\n",
    "    Returns a set of top-k Titles recommended by the given llm+encoder combo.\n",
    "    \"\"\"\n",
    "    idx_path = f\"expanded_interest_books_nxd_vllm_{llm_variant}_{encoder_variant}_faiss.index\"\n",
    "    df_path  = f\"expanded_interest_books_nxd_vllm_{llm_variant}.pkl\"\n",
    "    df = pd.read_pickle(df_path)\n",
    "    idx = faiss.read_index(idx_path)\n",
    "    # reconstruct row 0 as query\n",
    "    q = np.zeros((1, idx.d), dtype=\"float32\")\n",
    "    idx.reconstruct_n(0, 1, q)\n",
    "    D, I = idx.search(q, top_k)\n",
    "    return set(df.iloc[I[0]][\"Title\"].tolist())\n",
    "\n",
    "llm_variants     = [\"1b\", \"3b\", \"8b\", \"70b\"]\n",
    "encoder_variants = [\"t5_base\", \"t5_large\", \"t5_xl\"]\n",
    "top_k            = 5\n",
    "\n",
    "# 1) Overlap between encoders for each LLM\n",
    "for llm in llm_variants:\n",
    "    print(f\"\\nOverlap among encoders for LLM={llm}:\")\n",
    "    for e1, e2 in combinations(encoder_variants, 2):\n",
    "        set1 = recommend_books(\"\", llm, e1, top_k)\n",
    "        set2 = recommend_books(\"\", llm, e2, top_k)\n",
    "        common = set1 & set2\n",
    "        pct = 100 * len(common) / top_k\n",
    "        print(f\"  {e1} vs {e2}: {len(common)}/{top_k} ({pct:.1f}%)\")\n",
    "\n",
    "# 2) Overlap between LLMs for each encoder\n",
    "for enc in encoder_variants:\n",
    "    print(f\"\\nOverlap among LLMs for Encoder={enc}:\")\n",
    "    for m1, m2 in combinations(llm_variants, 2):\n",
    "        set1 = recommend_books(\"\", m1, enc, top_k)\n",
    "        set2 = recommend_books(\"\", m2, enc, top_k)\n",
    "        common = set1 & set2\n",
    "        pct = 100 * len(common) / top_k\n",
    "        print(f\"  {m1} vs {m2}: {len(common)}/{top_k} ({pct:.1f}%)\")\n",
    "\n",
    "# 3) Optional: plot a heatmap of encoder-overlaps for one LLM\n",
    "import seaborn as sns\n",
    "\n",
    "data = []\n",
    "labels = encoder_variants\n",
    "for e1, e2 in combinations(encoder_variants, 2):\n",
    "    data.append([\n",
    "        100 * len(recommend_books(\"\", llm_variants[2], e1, top_k) &\n",
    "                 recommend_books(\"\", llm_variants[2], e2, top_k)) / top_k\n",
    "    ])\n",
    "heat = np.zeros((len(labels), len(labels)))\n",
    "for i, e1 in enumerate(labels):\n",
    "    for j, e2 in enumerate(labels):\n",
    "        if i < j:\n",
    "            heat[i,j] = 100 * len(recommend_books(\"\", llm_variants[2], e1, top_k) &\n",
    "                                  recommend_books(\"\", llm_variants[2], e2, top_k)) / top_k\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(heat, xticklabels=labels, yticklabels=labels, annot=True, cmap=\"Blues\", vmin=0, vmax=100)\n",
    "plt.title(f\"Encoder Overlap (%) for LLM={llm_variants[2].upper()}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7c301f6-5333-4d87-98fe-cccf0cc89739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE recs:  {'The Hobbitt, or there and back again; illustrated by the author.', 'Newcomer Program Program (Prentice Hall Regents ESL)', 'Its Only Art If Its Well Hung!', 'The Secret of Red Gate Farm (Nancy Drew, Book 6)', 'Cruel and Unusual (G K Hall Large Print Book Series (Cloth))'}\n",
      "LARGE recs: {'God was in this place & I, i did not know: Finding self, spirituality, and ultimate meaning', 'Its Only Art If Its Well Hung!', 'Twenty Thousand Leagues Under the Sea (Caxton Edition)', 'Cruel and Unusual (G K Hall Large Print Book Series (Cloth))', 'The witch of blackbird pond (A Dell yearling book)'}\n",
      "Overlap:    {'Its Only Art If Its Well Hung!', 'Cruel and Unusual (G K Hall Large Print Book Series (Cloth))'}\n"
     ]
    }
   ],
   "source": [
    "base_recs  = recommend_books(\"\", \"8b\", \"t5_base\",  top_k=5)\n",
    "large_recs = recommend_books(\"\", \"8b\", \"t5_large\", top_k=5)\n",
    "\n",
    "print(\"BASE recs: \",  base_recs)\n",
    "print(\"LARGE recs:\", large_recs)\n",
    "print(\"Overlap:   \", base_recs & large_recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125920f-a46d-4421-b0fe-faa9e7427b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
