speculative_model: "meta-llama/Llama-3.2-1B"
speculative_draft_tensor_parallel_size: 1
speculative_max_model_len: 512
num_speculative_tokens: 7
model: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
tensor_parallel_size: 8
max_num_seqs: 1
max_model_len: 512
use_v2_block_manager: true
override_neuron_config: {"enable_fused_speculation":true} 
device: "neuron"
